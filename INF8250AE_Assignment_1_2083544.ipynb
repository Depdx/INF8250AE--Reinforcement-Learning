{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Depdx/INF8250AE--Reinforcement-Learning/blob/main/INF8250AE_Assignment_1_2083544.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3GnyazsiOXM",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "\n",
        "# Assignment 1\n",
        "\n",
        "## Instructions:\n",
        "* This is an individual assignment. You are not allowed to discuss the problems with other students.\n",
        "* Part of this assignment will be autograded by gradescope. You can use it as immediate feedback to improve your answers. You can resubmit as many times as you want.\n",
        "* All your solution, code, analysis, graphs, explanations should be done in this same notebook.\n",
        "* Please make sure to execute all the cells before you submit the notebook to the gradescope. You will not get points for the plots if they are not generated already.\n",
        "* If you have questions regarding the assignment, you can ask for clarifications in Piazza. You should use the corresponding tag for this assignment.\n",
        "\n",
        "\n",
        "**When Submitting to GradeScope**: Be sure to\n",
        "1) Submit a `.ipynb` notebook to the `Assignment 1 - Code` section on Gradescope.\n",
        "2) Submit a `pdf` version of the notebook to the `Assignment 1 - Report` entry and tag the answers.\n",
        "\n",
        "**Note**: You can choose to submit responses in either English or French.\n",
        "\n",
        "Before starting the assignment, make sure that you have downloaded all the tests related for the assignment and put them in the appropriate locations. If you run the next cell, we will set this all up automatically for you in a dataset called public, which will contain both the data and tests you use.\n",
        "\n",
        "This assignment has only one question. In this question, you will learn:\n",
        "\n",
        "1. To understand how to formalize a dose finding study as a multi-arm bandit problem.\n",
        "2. To implement **$\\epsilon$-greedy**, **UCB**, **Boltzmann**, and **Gradient bandit** algorithms.\n",
        "3. Understand the role of different hyper-parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iiv-lWb03xC8",
        "outputId": "b05a89c6-a608-4853-d470-0a7f128b5625"
      },
      "outputs": [],
      "source": [
        "!pip install -q otter-grader\n",
        "!git clone https://github.com/chandar-lab/INF8250ae-assignments-2023.git public"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TLspae33xC9"
      },
      "outputs": [],
      "source": [
        "import otter\n",
        "grader = otter.Notebook(colab=True, tests_dir='./public/a1/tests')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-_PFmea3xC-",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from random import choice, randint\n",
        "from scipy.stats import bernoulli\n",
        "from typing import Sequence, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "np.random.seed(8953)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NySaTSFe3xC-",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Q1: Dose Finding Study (90 points)\n",
        "\n",
        "In the context of clinical trials, Phase I trials are the first stage of testing in human subjects. Their goal is to evaluate the safety (and feasibility) of the treatment and identify its side effects. The aim of a phase I dose-finding study is to determine the most appropriate dose level that should be used in further phases of the clinical trials. Traditionally, the focus is on determining the highest dose with acceptable toxicity called the Maximum Tolerated Dose (MTD).\n",
        "\n",
        "A dose-finding study involves a number K of dose levels that have been chosen by physicians based on preliminary experiments (K is usually a number between 3 and 10). Denoting by $p_k$ the (unknown) toxicity probability of dose $k$, the Maximum Tolerated Dose (MTD) is defined as the dose with a toxicity probability closest to a target:\n",
        "\n",
        "\\begin{align}\n",
        "k^* \\in \\underset{k\\in\\{1,\\dots,K\\}}{\\mathrm{argmin}}|\\theta - p_k|\n",
        "\\end{align}\n",
        "\n",
        "where $\\theta$ is the pre-specified targeted toxicity probability (typically between 0.2 and 0.35).\n",
        "A MTD identification algorithm proceeds sequentially: at round $t$ a dose $D_t \\in \\{1, \\dots , K\\}$ is selected and administered to a patient for whom a toxicity response is observed. A binary outcome $X_t$ is revealed where $X_t = 1$ indicates that a harmful side-effect occurred and $X_t = 0$ indicates than no harmful side-effect occurred. We assume that $X_t$ is drawn from a Bernoulli distribution with mean $p_{D_t}$ and is independent from previous observations.\n",
        "\n",
        "**Hint**: In this example, the reward definition is a bit different from the usual case. We would like to take the arm with minimum $|\\theta - \\hat{p}_k|$ where $\\hat{p}_k$ is the estimated toxicity probability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "G5fFSo9rgEcW",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Q1.a: Define your Bandit class (5 points):\n",
        "\n",
        "Most of the class has been written. Complete the pull method in such a way that:\n",
        "\n",
        "**1.** Update both `num_dose_selected` and `num_toxic` arrays,\n",
        "\n",
        "**2.** Compute and return the reward $-|\\theta - \\hat{p}_k|$ where $\\hat{p}_k$ is the estimated toxicity probability of arm $k$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "192vo5l-YiWz",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Bandit(object):\n",
        "\n",
        "  def __init__(self,\n",
        "               n_arm: int = 2,\n",
        "               n_pulls: int = 2000,\n",
        "               actual_toxicity_prob: list = [0.4, 0.6],\n",
        "               theta: float = 0.3,\n",
        "               ):\n",
        "    self.n_arm = n_arm\n",
        "    self.n_pulls = n_pulls\n",
        "    self.actual_toxicity_prob = actual_toxicity_prob\n",
        "    self.theta = theta\n",
        "    # ----------------------------------------------\n",
        "    self.a_star = np.argmax([-abs(theta - p) for p in actual_toxicity_prob])\n",
        "    # ----------------------------------------------\n",
        "    self.init_bandit()\n",
        "\n",
        "  def init_bandit(self):\n",
        "    \"\"\"\n",
        "        Initialize the bandit\n",
        "    \"\"\"\n",
        "    self.num_dose_selected = np.array([0]*self.n_arm) # number of times a dose is selected\n",
        "    self.num_toxic = np.array([0]*self.n_arm) # number of times a does found to be toxic\n",
        "\n",
        "  def pull(self, a_idx: int):\n",
        "    \"\"\"\n",
        "    .inputs:\n",
        "      a_idx: Index of action.\n",
        "    .outputs:\n",
        "      rew: reward value.\n",
        "    \"\"\"\n",
        "    assert a_idx < self.n_arm, \"invalid action index\"\n",
        "    # ----------------------------------------------\n",
        "    self.num_dose_selected[a_idx] = self.num_dose_selected[a_idx] + 1\n",
        "    is_toxic = bernoulli.rvs(self.actual_toxicity_prob[a_idx])\n",
        "    self.num_toxic[a_idx] = self.num_toxic[a_idx] + is_toxic\n",
        "    p_hat = self.num_toxic[a_idx]/self.num_dose_selected[a_idx]\n",
        "    rew = -abs(self.theta - p_hat)\n",
        "    # ----------------------------------------------\n",
        "    return rew"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "deletable": false,
        "editable": false,
        "id": "2XvOSrua3xC_",
        "outputId": "f13bd936-1117-40fa-966b-2418146e8f81"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q1a\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "pOzYYAmygGoW",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Dose finding study with three doses\n",
        "\n",
        "Let's define a dose finding study with three doses ($K = 3$) where you need to choose from with `actual_toxicity_prob=[0.1, 0.35, 0.8]` and targeted toxicity probability is $\\theta = 0.3$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "emYdGDYS2HvG",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "#@title Problem definition\n",
        "bandit = Bandit(n_arm=3, n_pulls=2000, actual_toxicity_prob=[0.1, 0.35, 0.8], theta=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "sDgjH3NoiOXU",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Q1.b: $\\epsilon$-greedy for k-armed bandit and Optimistic initial values (25 points)\n",
        "\n",
        "#### Q1.b1: $\\epsilon$-greedy algorithm implementation (5 points)\n",
        "\n",
        "Implement the $\\epsilon$-greedy method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t-G3rt5eCyM",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def eps_greedy(\n",
        "    bandit: Bandit,\n",
        "    eps: float,\n",
        "    init_q: float = .0\n",
        "    ) -> Tuple[list, list, list]:\n",
        "  \"\"\"\n",
        "  .inputs:\n",
        "    bandit: A bandit problem, instantiated from the above class.\n",
        "    eps: The epsilon value.\n",
        "    init_q: Initial estimation of each arm's value.\n",
        "  .outputs:\n",
        "    rew_record: The record of rewards at each timestep.\n",
        "    avg_ret_record: The average of rewards up to step t, where t goes from 0 to n_pulls. For example: If\n",
        "    we define `ret_T` = \\sum^T_{t=0}{r_t}, `avg_ret_record` = ret_T / (1+T).\n",
        "    tot_reg_record: The  regret up to step t, where t goes from 0 to n_pulls.\n",
        "    opt_action_perc_record: Percentage of optimal arm selected.\n",
        "  \"\"\"\n",
        "  # initialize q values\n",
        "  q = np.array([init_q]*bandit.n_arm, dtype=float)\n",
        "\n",
        "  ret = .0\n",
        "  rew_record = []\n",
        "  avg_ret_record = []\n",
        "  tot_reg_record = []\n",
        "  opt_action_perc_record = []\n",
        "\n",
        "  for t in range(bandit.n_pulls):\n",
        "    # ----------------------------------------------\n",
        "    explore = np.random.binomial(1, eps)\n",
        "    a:int = None\n",
        "    if explore:\n",
        "      a = randint(0, len(q)-1)\n",
        "    else:\n",
        "      a = np.argmax(q)\n",
        "\n",
        "    rew = bandit.pull(a)\n",
        "    q[a] = q[a] + (rew - q[a]) / (bandit.num_dose_selected[a])\n",
        "    rew_record.append(rew)\n",
        "    ret += rew\n",
        "    avg_ret_record.append(ret / (t+1))\n",
        "\n",
        "    tot_reg_record.append((t+1) * bandit.theta - ret)\n",
        "\n",
        "    opt_action_perc = np.sum(bandit.num_dose_selected[bandit.a_star])/np.sum(bandit.num_dose_selected)\n",
        "    opt_action_perc_record.append(opt_action_perc)\n",
        "    # ----------------------------------------------\n",
        "\n",
        "  return rew_record, avg_ret_record, tot_reg_record, opt_action_perc_record\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "deletable": false,
        "editable": false,
        "id": "QtDy06h_3xDB",
        "outputId": "bef914f4-ca76-44cc-ce5f-121945063034"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q1b1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Knx7oUDmt0rf",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "#### Q1.b2: Plotting the results (5 points)\n",
        "\n",
        "Use the driver code provided to plot: (1) The average return, (2) The reward, (3) the total regret, and (4) the percentage of optimal action across the $N$=20 runs as a function of the number of pulls (2000 pulls for each run) for all three $\\epsilon$ values of 0.5, 0.1, and 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "c4IeQOe9oQub",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import time\n",
        "plt.figure(0)\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"avg return\")\n",
        "plt.figure(1)\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"reward\")\n",
        "plt.figure(2)\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"total regret\")\n",
        "plt.figure(3)\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"% optimal action\")\n",
        "\n",
        "N = 20\n",
        "tot_reg_rec_best = 1e8\n",
        "\n",
        "for eps in [0.5, 0.1, .0]:\n",
        "  rew_rec = np.zeros(bandit.n_pulls)\n",
        "  avg_ret_rec = np.zeros(bandit.n_pulls)\n",
        "  tot_reg_rec = np.zeros(bandit.n_pulls)\n",
        "  opt_act_rec = np.zeros(bandit.n_pulls)\n",
        "  start_time = time.time()\n",
        "  for n in range(N):\n",
        "    bandit.init_bandit()\n",
        "    rew_rec_n, avg_ret_rec_n, tot_reg_rec_n, opt_act_rec_n = eps_greedy(bandit, eps)\n",
        "    rew_rec += np.array(rew_rec_n)\n",
        "    avg_ret_rec += np.array(avg_ret_rec_n)\n",
        "    tot_reg_rec += np.array(tot_reg_rec_n)\n",
        "    opt_act_rec += np.array(opt_act_rec_n)\n",
        "\n",
        "  end_time = time.time()\n",
        "  # take the mean\n",
        "  rew_rec /= N\n",
        "  avg_ret_rec /= N\n",
        "  tot_reg_rec /= N\n",
        "  opt_act_rec /= N\n",
        "\n",
        "  plt.figure(0)\n",
        "  plt.plot(avg_ret_rec, label=\"eps={}\".format(eps))\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "  plt.figure(1)\n",
        "  plt.plot(rew_rec[1:], label=\"eps={}\".format(eps))\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "  plt.figure(2)\n",
        "  plt.plot(tot_reg_rec, label=\"eps={}\".format(eps))\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "  plt.figure(3)\n",
        "  plt.plot(opt_act_rec, label=\"eps={}\".format(eps))\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "\n",
        "  if tot_reg_rec[-1] < tot_reg_rec_best:\n",
        "        ep_greedy_dict = {\n",
        "        'opt_act':opt_act_rec,\n",
        "        'regret_list':tot_reg_rec,}\n",
        "        tot_reg_rec_best = tot_reg_rec[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "zlBSBgi03xDC",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "#### Q1.b3: Analysis (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "FzebIVY73xDC",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "\n",
        "Explain the results from the perspective of exploration and how different $\\epsilon$ values affect the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Wikk6gmY3xDC",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Type your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "xKovsXgW3xDC",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "#### Q1.b4: Optimistic Initial Values (5 points)\n",
        "We want to run the optimistic initial value method on the same problem described above for the initial q values of -1 and +1 for all arms. Compare its performance, measured by the average reward across $N$=20 runs as a function of the number of pulls, with the non-optimistic setting with initial q values of 0 for all arms. For both optimistic and non-optimistic settings, $\\epsilon$=0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Kv0leL733xDC",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(4)\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"avg return\")\n",
        "\n",
        "plt.figure(5)\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"reward\")\n",
        "\n",
        "plt.figure(6)\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"total regret\")\n",
        "\n",
        "N = 20\n",
        "for init_q in [-1, 1]:\n",
        "  rew_rec = np.zeros(bandit.n_pulls)\n",
        "  avg_ret_rec = np.zeros(bandit.n_pulls)\n",
        "  for n in range(N):\n",
        "    bandit.init_bandit()\n",
        "    rew_rec_n, avg_ret_rec_n, tot_reg_rec_n, opt_act_rec_n = eps_greedy(bandit, eps=0.0, init_q=init_q)\n",
        "\n",
        "    rew_rec += np.array(rew_rec_n)\n",
        "    avg_ret_rec += np.array(avg_ret_rec_n)\n",
        "    tot_reg_rec += np.array(tot_reg_rec_n)\n",
        "\n",
        "  avg_ret_rec /= N\n",
        "  rew_rec /= N\n",
        "  tot_reg_rec /= N\n",
        "  plt.figure(4)\n",
        "  plt.plot(avg_ret_rec[1:], label=\"q_init_val={}\".format(init_q))\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "  plt.figure(5)\n",
        "  plt.plot(rew_rec[1:], label=\"q_init_val={}\".format(init_q))\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "  plt.figure(6)\n",
        "  plt.plot(tot_reg_rec[1:], label=\"q_init_val={}\".format(init_q))\n",
        "  plt.legend(loc=\"lower right\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "fF9_g8FW3xDD",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "#### Q1.b5: Analysis (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "P3oy1Qec3xDD",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Explain how initial q values affect the exploration and the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "HP5lUnwH3xDD",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Type your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "nJe1Q-0ueaY7",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "### Q1.c: Upper-Confidence-Bound action selection (15 points)\n",
        "\n",
        "#### Q1.c1: UCB algorithm implementation (5 points)\n",
        "Implement the UCB algorithm on the same MAB problem as above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWDDMQO4T0EO",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def ucb(\n",
        "    bandit: Bandit,\n",
        "    c: float,\n",
        "    init_q: float = .0\n",
        "    ) -> Tuple[list, list, list]:\n",
        "  \"\"\"\n",
        "  .inputs:\n",
        "    bandit: A bandit problem, instantiated from the above class.\n",
        "    c: The additional term coefficient.\n",
        "    init_q: Initial estimation of each arm's value.\n",
        "  .outputs:\n",
        "    rew_record: The record of rewards at each timestep.\n",
        "    avg_ret_record: The average summation of rewards up to step t, where t goes from 0 to n_pulls. For example: If\n",
        "    we define `ret_T` = \\sum^T_{t=0}{r_t}, `avg_ret_record` = ret_T / (1+T).\n",
        "    tot_reg_record: The  regret up to step t, where t goes from 0 to n_pulls.\n",
        "    opt_action_perc_record: Percentage of optimal arm selected.\n",
        "  \"\"\"\n",
        "  # init q values (the estimates)\n",
        "  q = np.array([init_q]*bandit.n_arm, dtype=float)\n",
        "\n",
        "  ret = .0\n",
        "  rew_record = []\n",
        "  avg_ret_record = []\n",
        "  tot_reg_record = []\n",
        "  opt_action_perc_record = []\n",
        "\n",
        "  for t in range(bandit.n_pulls):\n",
        "    # Assuming to take the first arm always when there is no exploration\n",
        "    # ----------------------------------------------\n",
        "    ucb = []\n",
        "    for a in range(bandit.n_arm):\n",
        "        if bandit.num_dose_selected[a] == 0:\n",
        "            ucb.append(float('inf'))\n",
        "        else:\n",
        "            ucb.append(q[a] + c * np.sqrt(np.log(t+1) / (bandit.num_dose_selected[a])))\n",
        "\n",
        "    a = np.argmax(ucb)\n",
        "\n",
        "    rew = bandit.pull(a)\n",
        "    q[a] = q[a] + (rew - q[a]) / (bandit.num_dose_selected[a])\n",
        "    rew_record.append(rew)\n",
        "    ret += rew\n",
        "    avg_ret_record.append(ret / (t+1))\n",
        "\n",
        "    tot_reg_record.append((t+1) * bandit.theta - ret)\n",
        "\n",
        "    opt_action_perc = np.sum(bandit.num_dose_selected[bandit.a_star])/np.sum(bandit.num_dose_selected)\n",
        "    opt_action_perc_record.append(opt_action_perc)\n",
        "    # ----------------------------------------------\n",
        "\n",
        "  return rew_record, avg_ret_record, tot_reg_record, opt_action_perc_record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "1zgQrnXV3xDD"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q1c1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njJJcTBvmqq1",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "#### Q1.c2: Plotting the results (5 points)\n",
        "\n",
        "Use the driver code provided to plot: (1) The average return, (2) The reward, (3) the total regret, and (4) the percentage of optimal action across the $N$=20 runs as a function of the number of pulls (2000 pulls for each run) for three values of $c$=0, 0.5, and 2.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjsHOdnfmkGP",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(7)\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"avg return\")\n",
        "plt.figure(8)\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"reward\")\n",
        "plt.figure(9)\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"total regret\")\n",
        "plt.figure(10)\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"% optimal action\")\n",
        "\n",
        "N = 20\n",
        "tot_reg_rec_best = 1e8\n",
        "for c in [.0, 0.5, 2]:\n",
        "  rew_rec = np.zeros(bandit.n_pulls)\n",
        "  avg_ret_rec = np.zeros(bandit.n_pulls)\n",
        "  tot_reg_rec = np.zeros(bandit.n_pulls)\n",
        "  opt_act_rec = np.zeros(bandit.n_pulls)\n",
        "\n",
        "  for n in range(N):\n",
        "    bandit.init_bandit()\n",
        "    rew_rec_n, avg_ret_rec_n, tot_reg_rec_n, opt_act_rec_n = ucb(bandit, c)\n",
        "    rew_rec += np.array(rew_rec_n)\n",
        "    avg_ret_rec += np.array(avg_ret_rec_n)\n",
        "    tot_reg_rec += np.array(tot_reg_rec_n)\n",
        "    opt_act_rec += np.array(opt_act_rec_n)\n",
        "\n",
        "  # take the mean\n",
        "  rew_rec /= N\n",
        "  avg_ret_rec /= N\n",
        "  tot_reg_rec /= N\n",
        "  opt_act_rec /= N\n",
        "\n",
        "  plt.figure(7)\n",
        "  plt.plot(avg_ret_rec, label=\"c={}\".format(c))\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "  plt.figure(8)\n",
        "  plt.plot(rew_rec, label=\"c={}\".format(c))\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "  plt.figure(9)\n",
        "  plt.plot(tot_reg_rec, label=\"c={}\".format(c))\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "  plt.figure(10)\n",
        "  plt.plot(opt_act_rec, label=\"c={}\".format(c))\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "  if tot_reg_rec[-1] < tot_reg_rec_best:\n",
        "        ucb_dict = {\n",
        "        'opt_act':opt_act_rec,\n",
        "        'regret_list':tot_reg_rec,}\n",
        "        tot_reg_rec_best = tot_reg_rec[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnNlfgt53xDE",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "#### Q1.c3: Analysis (5 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XamA6umJnR0d",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        " Explain the results from the perspective of exploration and how different $c$ values affect the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YFOW9T-3xDE",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Type your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8wr6xzewiOXa",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Q1.d: Boltzmann algorithm (20 points)\n",
        "\n",
        "#### Q1.d1: Boltzmann policy implementation (5 points)\n",
        "\n",
        "Implement a Boltzmann policy that gets an array and temprature value ($\\tau$) and returns an index sampled from the Boltzmann policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLuMx_xFiOXb",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def boltzmann_policy(x, tau):\n",
        "    \"\"\" Returns softmax probabilities with temperature tau\n",
        "        Input:  x -- 1-dimensional array\n",
        "        Output: idx -- chosen index\n",
        "    \"\"\"\n",
        "    # ----------------------------------------------\n",
        "    x = np.array(x)\n",
        "    exp_x = np.exp(x/tau)\n",
        "    p = exp_x / np.sum(exp_x)\n",
        "    idx = np.random.choice(len(x), p=p)\n",
        "    # ----------------------------------------------\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "yDXPqkU93xDE"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q1d1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "kYLEgjSYuGoY",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "#### Q1.d2: Boltzmann algorithm implementation (5 points)\n",
        "\n",
        "Evaluate the Boltzmann algorithm on the same MAB problem as above, for three values of the parameters $\\tau$: $0.01$, $0.1$, and $1$. Use the driver code provided to plot their performances across $N$=20 runs as a function of the number of pulls.\n",
        "\n",
        "**Note:** You can use action-value estimates for the Boltzmann distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzlVfqvGiOXb",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def boltzmann(\n",
        "    bandit: Bandit,\n",
        "    tau: float = 0.1,\n",
        "    init_q: float = .0\n",
        "    ) -> Tuple[list, list, list]:\n",
        "  \"\"\"\n",
        "  .inputs:\n",
        "    bandit: A bandit problem, instantiated from the above class.\n",
        "    tau: The additional term coefficient.\n",
        "    init_q: Initial estimation of each arm's value.\n",
        "  .outputs:\n",
        "    rew_record: The record of rewards at each timestep.\n",
        "    avg_ret_record: The average summation of rewards up to step t, where t goes from 0 to n_pulls. For example: If\n",
        "    we define `ret_T` = \\sum^T_{t=0}{r_t}, `avg_ret_record` = ret_T / (1+T).\n",
        "    tot_reg_record: The  regret up to step t, where t goes from 0 to n_pulls.\n",
        "    opt_action_perc_record: Percentage of optimal arm selected.\n",
        "  \"\"\"\n",
        "  # init q values (the estimates)\n",
        "  q = np.array([init_q]*bandit.n_arm, dtype=float)\n",
        "\n",
        "  ret = .0\n",
        "  rew_record = []\n",
        "  avg_ret_record = []\n",
        "  tot_reg_record = []\n",
        "  opt_action_perc_record = []\n",
        "\n",
        "  for t in range(bandit.n_pulls):\n",
        "    # ----------------------------------------------\n",
        "    a = boltzmann_policy(q, tau)\n",
        "\n",
        "    rew = bandit.pull(a)\n",
        "    q[a] = q[a] + (rew - q[a]) / (bandit.num_dose_selected[a])\n",
        "    rew_record.append(rew)\n",
        "    ret += rew\n",
        "    avg_ret_record.append(ret / (t+1))\n",
        "\n",
        "    tot_reg_record.append((t+1) * bandit.theta - ret)\n",
        "\n",
        "    opt_action_perc = np.sum(bandit.num_dose_selected[bandit.a_star])/np.sum(bandit.num_dose_selected)\n",
        "    opt_action_perc_record.append(opt_action_perc)\n",
        "    # ----------------------------------------------\n",
        "\n",
        "  return rew_record, avg_ret_record, tot_reg_record, opt_action_perc_record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "MKwmVlIF3xDF"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q1d2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8jk1IcO3xDF",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "#### Q1.d3: Plotting the results (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWarVGeKiOXb",
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "plt.figure(11)\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"avg return\")\n",
        "plt.figure(12)\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"reward\")\n",
        "plt.figure(13)\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"total regret\")\n",
        "plt.figure(14)\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"% optimal action\")\n",
        "\n",
        "N = 20\n",
        "tot_reg_rec_best = 1e8\n",
        "for tau in [0.01, 0.1, 1]:\n",
        "  rew_rec = np.zeros(bandit.n_pulls)\n",
        "  avg_ret_rec = np.zeros(bandit.n_pulls)\n",
        "  tot_reg_rec = np.zeros(bandit.n_pulls)\n",
        "  opt_act_rec = np.zeros(bandit.n_pulls)\n",
        "\n",
        "  for n in range(N):\n",
        "    bandit.init_bandit()\n",
        "    rew_rec_n, avg_ret_rec_n, tot_reg_rec_n, opt_act_rec_n = boltzmann(bandit, tau=tau)\n",
        "    rew_rec += np.array(rew_rec_n)\n",
        "    avg_ret_rec += np.array(avg_ret_rec_n)\n",
        "    tot_reg_rec += np.array(tot_reg_rec_n)\n",
        "    opt_act_rec += np.array(opt_act_rec_n)\n",
        "\n",
        "  # take the mean\n",
        "  rew_rec /= N\n",
        "  avg_ret_rec /= N\n",
        "  tot_reg_rec /= N\n",
        "  opt_act_rec /= N\n",
        "\n",
        "  plt.figure(11)\n",
        "  plt.plot(avg_ret_rec, label=\"tau={}\".format(tau))\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "  plt.figure(12)\n",
        "  plt.plot(rew_rec, label=\"tau={}\".format(tau))\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "  plt.figure(13)\n",
        "  plt.plot(tot_reg_rec, label=\"tau={}\".format(tau))\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "  plt.figure(14)\n",
        "  plt.plot(opt_act_rec, label=\"tau={}\".format(tau))\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "  if tot_reg_rec[-1] < tot_reg_rec_best:\n",
        "        boltzmann_dict = {\n",
        "        'opt_act':opt_act_rec,\n",
        "        'regret_list':tot_reg_rec,}\n",
        "        tot_reg_rec_best = tot_reg_rec[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdnZzCtX3xDJ",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "#### Q1.d4: Analysis (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyWkUoql3xDJ",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        " Explain the role of $\\tau$ paramtere on the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMh1LHOY3xDJ",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Type your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "w-R2l61S3xDJ"
      },
      "source": [
        "### Q1.f: Gradient Bandits Algorithm (15 points)\n",
        "\n",
        "#### Q1.f1: GB implementation  (5 points)\n",
        "Follow the lecture notes to implement the Gradient Bandits algorithm with and without the baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrNFvgzj3xDJ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsEsumbo3xDJ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def gradient_bandit(\n",
        "    bandit: Bandit,\n",
        "    alpha: float,\n",
        "    use_baseline: bool = True,\n",
        "    ) -> Tuple[list, list, list]:\n",
        "  \"\"\"\n",
        "  .inputs:\n",
        "    bandit: A bandit problem, instantiated from the above class.\n",
        "    alpha: The learning rate.\n",
        "    use_baseline: Whether or not use avg return as baseline.\n",
        "  .outputs:\n",
        "    rew_record: The record of rewards at each timestep.\n",
        "    avg_ret_record: The average summation of rewards up to step t, where t goes from 0 to n_pulls. For example: If\n",
        "    we define `ret_T` = \\sum^T_{t=0}{r_t}, `avg_ret_record` = ret_T / (1+T).\n",
        "    tot_reg_record: The  regret up to step t, where t goes from 0 to n_pulls.\n",
        "    opt_action_perc_record: Percentage of optimal arm selected.\n",
        "  \"\"\"\n",
        "  # init h (the logits)\n",
        "  h = np.array([0]*bandit.n_arm, dtype=float)\n",
        "\n",
        "  ret = .0\n",
        "  r_bar_t = 0\n",
        "  rew_record = []\n",
        "  avg_ret_record = []\n",
        "  tot_reg_record = []\n",
        "  opt_action_perc_record = []\n",
        "\n",
        "\n",
        "  for t in range(bandit.n_pulls):\n",
        "    # ----------------------------------------------\n",
        "    p = softmax(h)\n",
        "    a = np.random.choice(bandit.n_arm, p=p)\n",
        "\n",
        "    rew = bandit.pull(a)\n",
        "\n",
        "    if use_baseline:\n",
        "        r_bar_t = r_bar_t + (rew - r_bar_t) / (t+1)\n",
        "        for i in range(len(h)):\n",
        "            if i == a:\n",
        "                h[a] = h[a] + alpha * (rew - r_bar_t) * p[a] * (1 - p[a])\n",
        "            else:\n",
        "                h[i] = h[i] - alpha * (rew - r_bar_t) * p[a] * p[i]\n",
        "    else:\n",
        "        for i in range(len(h)):\n",
        "            if i == a:\n",
        "                h[a] = h[a] + alpha * rew * p[a] * (1 - p[a])\n",
        "            else:\n",
        "                h[i] = h[i] - alpha * rew * p[a] * p[i]\n",
        "\n",
        "    rew_record.append(rew)\n",
        "    ret += rew\n",
        "    avg_ret_record.append(ret / (t+1))\n",
        "\n",
        "    tot_reg_record.append((t+1) * bandit.theta - ret)\n",
        "\n",
        "    opt_action_perc = np.sum(bandit.num_dose_selected[bandit.a_star])/np.sum(bandit.num_dose_selected)\n",
        "    opt_action_perc_record.append(opt_action_perc)\n",
        "    # ----------------------------------------------\n",
        "\n",
        "  return rew_record, avg_ret_record, tot_reg_record, opt_action_perc_record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "pK0A8seR3xDK"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q1f1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_gfd3FM3xDK"
      },
      "source": [
        "#### Q1.f2: Plotting the results (5 points)\n",
        "\n",
        "Evaluate the GB algorithm on the same MAB problem as above, for three values of the parameters $\\alpha$: $0.05$, $0.1$, and $2$. Use the driver code provided to plot their performances.\n",
        "\n",
        "**With baseline:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6BewyQ33xDK"
      },
      "outputs": [],
      "source": [
        "plt.figure(15)\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"avg return\")\n",
        "plt.figure(16)\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"reward\")\n",
        "plt.figure(17)\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"total regret\")\n",
        "plt.figure(18)\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"% optimal action\")\n",
        "\n",
        "N = 20\n",
        "tot_reg_rec_best = 1e8\n",
        "for alpha in [0.05, 0.1, 2]:\n",
        "  rew_rec = np.zeros(bandit.n_pulls)\n",
        "  avg_ret_rec = np.zeros(bandit.n_pulls)\n",
        "  tot_reg_rec = np.zeros(bandit.n_pulls)\n",
        "  opt_act_rec = np.zeros(bandit.n_pulls)\n",
        "\n",
        "  for n in range(N):\n",
        "    bandit.init_bandit()\n",
        "    rew_rec_n, avg_ret_rec_n, tot_reg_rec_n, opt_act_rec_n = gradient_bandit(bandit, alpha=alpha)\n",
        "    rew_rec += np.array(rew_rec_n)\n",
        "    avg_ret_rec += np.array(avg_ret_rec_n)\n",
        "    tot_reg_rec += np.array(tot_reg_rec_n)\n",
        "    opt_act_rec += np.array(opt_act_rec_n)\n",
        "\n",
        "\n",
        "  # take the mean\n",
        "  rew_rec /= N\n",
        "  avg_ret_rec /= N\n",
        "  tot_reg_rec /= N\n",
        "\n",
        "  plt.figure(15)\n",
        "  plt.plot(avg_ret_rec, label=\"alpha={}\".format(alpha))\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "  plt.figure(16)\n",
        "  plt.plot(rew_rec, label=\"alpha={}\".format(alpha))\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "  plt.figure(17)\n",
        "  plt.plot(tot_reg_rec, label=\"alpha={}\".format(alpha))\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "  plt.figure(18)\n",
        "  plt.plot(opt_act_rec, label=\"alpha={}\".format(alpha))\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "  if tot_reg_rec[-1] < tot_reg_rec_best:\n",
        "        gradient_bandit_dict = {\n",
        "        'opt_act':opt_act_rec,\n",
        "        'regret_list':tot_reg_rec,}\n",
        "        tot_reg_rec_best = tot_reg_rec[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMA0gArE3xDK"
      },
      "source": [
        "**Without baseline:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV7bXATY3xDK"
      },
      "outputs": [],
      "source": [
        "plt.figure(19)\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"avg return\")\n",
        "plt.figure(20)\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"reward\")\n",
        "plt.figure(21)\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"total regret\")\n",
        "plt.figure(22)\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"% optimal action\")\n",
        "\n",
        "N = 20\n",
        "tot_reg_rec_best = 1e8\n",
        "for alpha in [0.05, 0.1, 2]:\n",
        "  rew_rec = np.zeros(bandit.n_pulls)\n",
        "  avg_ret_rec = np.zeros(bandit.n_pulls)\n",
        "  tot_reg_rec = np.zeros(bandit.n_pulls)\n",
        "  opt_act_rec = np.zeros(bandit.n_pulls)\n",
        "\n",
        "  for n in range(N):\n",
        "    bandit.init_bandit()\n",
        "    rew_rec_n, avg_ret_rec_n, tot_reg_rec_n, opt_act_rec_n = gradient_bandit(bandit, alpha=alpha, use_baseline=False)\n",
        "    rew_rec += np.array(rew_rec_n)\n",
        "    avg_ret_rec += np.array(avg_ret_rec_n)\n",
        "    tot_reg_rec += np.array(tot_reg_rec_n)\n",
        "    opt_act_rec += np.array(opt_act_rec_n)\n",
        "\n",
        "\n",
        "  # take the mean\n",
        "  rew_rec /= N\n",
        "  avg_ret_rec /= N\n",
        "  tot_reg_rec /= N\n",
        "  opt_act_rec /= N\n",
        "\n",
        "  plt.figure(19)\n",
        "  plt.plot(avg_ret_rec, label=\"alpha={}\".format(alpha))\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "  plt.figure(20)\n",
        "  plt.plot(rew_rec, label=\"alpha={}\".format(alpha))\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "  plt.figure(21)\n",
        "  plt.plot(tot_reg_rec, label=\"alpha={}\".format(alpha))\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "  plt.figure(22)\n",
        "  plt.plot(opt_act_rec, label=\"alpha={}\".format(alpha))\n",
        "  plt.legend(loc=\"lower right\")\n",
        "\n",
        "  if tot_reg_rec[-1] < tot_reg_rec_best:\n",
        "        gradient_bandit_dict = {\n",
        "        'opt_act':opt_act_rec,\n",
        "        'regret_list':tot_reg_rec,}\n",
        "        tot_reg_rec_best = tot_reg_rec[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uki4LK6w3xDK"
      },
      "source": [
        "#### Q1.f3: Analysis (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydaaeM7C3xDL"
      },
      "source": [
        "Explain the role of $\\alpha$ and the baseline on the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZRt3Ewt3xDL"
      },
      "source": [
        "Type your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "zL1yjknB3xDL"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "### Q1.g: Final comaprison (10 points)\n",
        "\n",
        "#### Q1.g1: plots (5 points)\n",
        "Compare the performance of $\\epsilon$-greedy, UCB, Boltzmann algorithm, and Gradient Bandit algorithm in a single plot as measured by the average reward and total regret."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "4MNgamAz3xDL"
      },
      "outputs": [],
      "source": [
        "plt.figure(23)\n",
        "plt.plot(ep_greedy_dict[\"opt_act\"], label=\"e-greedy\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.plot(ucb_dict[\"opt_act\"], label=\"UCB\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.plot(boltzmann_dict[\"opt_act\"], label=\"Boltzmann\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.plot(gradient_bandit_dict[\"opt_act\"], label=\"GB\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"% optimal action\")\n",
        "\n",
        "plt.figure(24)\n",
        "plt.plot(ep_greedy_dict[\"regret_list\"], label=\"e-greedy\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.plot(ucb_dict[\"regret_list\"], label=\"UCB\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.plot(boltzmann_dict[\"regret_list\"], label=\"Boltzmann\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.plot(gradient_bandit_dict[\"regret_list\"], label=\"GB\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.xlabel(\"n pulls\")\n",
        "plt.ylabel(\"total regret\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "KAnOdrTc3xDL",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "#### Q1.g2: Analysis (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "UBOgS9Qu3xDL",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Compare all the algorithms in terms of their performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "419Yu9kT3xDL",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Type your answer here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "xgnbmtKk3xDM",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "plt.close('all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Nj9omiuU3xDM"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
