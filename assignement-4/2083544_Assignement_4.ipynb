{"cells":[{"cell_type":"markdown","metadata":{"id":"HTcTWPbRDXkm"},"source":["# **Assignment 4** (114 pts)\n","# Policy Gradients\n","---\n","---\n","## **Instructions**\n","* This is an individual assignment. You are **not allowed** to discuss the problems with other students.\n","* Part of this assignment will be autograded by gradescope. You can use it as immediate feedback to improve your answers. You can resubmit as many times as you want.\n","* All your solution, code, analysis, graphs, explanations should be done in this same notebook.\n","* Please make sure to execute all the cells before you submit the notebook to the gradescope. You will not get points for the plots if they are not generated already.\n","* Please **do not** change the random seeds\n","* Please start early. Some of the experiments take a lot of time to run on CPU.\n","* If you have questions regarding the assignment, you can ask for clarifications on Piazza. You should use the corresponding tag for this assignment.\n","* The deadline for submitting this assignment is **10:00 PM on Sunday,** **November 26, 2023**\n","---\n","---\n","\n","This assignment has 4 parts. The goals of these parts are:\n","- **Part 1**: Implementing a parameterized (neural network) policy with PyTorch\n","- **Part 2**: Understanding and implementing the REINFORCE algorithm\n","- **Part 3**: Extending the REINFORCE algorithm with a baseline\n","- **Part 4**: Understanding and implementing Actor-Critic\n","\n","**When Submitting to GradeScope**: Be sure to\n","1. Submit a .ipynb notebook to the Assignment 4 - Code section on Gradescope.\n","2. Submit a pdf version of the notebook to the Assignment 4 - Report entry.\n","\n","Note: You can choose to submit responses in either English or French.\n","\n","Before starting the assignment, make sure that you have downloaded all the tests related\n","for the assignment and put them in the appropriate locations. If you run the next cell,\n","we will set this all up automatically for you in a dataset called public, which contains the test cases.\n","\n","### Installing Dependencies"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"UjKljKv4KeoM"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: otter-grader in /usr/local/lib/python3.10/dist-packages (5.2.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from otter-grader) (0.3.7)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from otter-grader) (3.1.2)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from otter-grader) (5.9.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from otter-grader) (1.5.3)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from otter-grader) (6.0.1)\n","Requirement already satisfied: python-on-whales in /usr/local/lib/python3.10/dist-packages (from otter-grader) (0.65.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from otter-grader) (2.31.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from otter-grader) (1.15.0)\n","Requirement already satisfied: jupytext in /usr/local/lib/python3.10/dist-packages (from otter-grader) (1.15.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from otter-grader) (8.1.7)\n","Requirement already satisfied: fica>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from otter-grader) (0.3.1)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from otter-grader) (7.34.0)\n","Requirement already satisfied: astunparse in /usr/local/lib/python3.10/dist-packages (from otter-grader) (1.6.3)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from otter-grader) (7.7.1)\n","Requirement already satisfied: ipylab in /usr/local/lib/python3.10/dist-packages (from otter-grader) (1.0.0)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from otter-grader) (6.5.4)\n","Requirement already satisfied: docutils in /usr/local/lib/python3.10/dist-packages (from fica>=0.3.0->otter-grader) (0.18.1)\n","Requirement already satisfied: sphinx in /usr/local/lib/python3.10/dist-packages (from fica>=0.3.0->otter-grader) (5.0.2)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse->otter-grader) (0.41.2)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse->otter-grader) (1.16.0)\n","Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->otter-grader) (6.26.0)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->otter-grader) (0.2.0)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->otter-grader) (5.7.1)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->otter-grader) (3.6.5)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->otter-grader) (3.0.8)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (67.7.2)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (0.19.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (3.0.39)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (0.1.6)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (4.8.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->otter-grader) (2.1.3)\n","Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from jupytext->otter-grader) (0.10.2)\n","Requirement already satisfied: markdown-it-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from jupytext->otter-grader) (3.0.0)\n","Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.10/dist-packages (from jupytext->otter-grader) (0.4.0)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (4.9.3)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (4.11.2)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (6.0.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (0.7.1)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (0.4)\n","Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (5.3.1)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (0.2.2)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (0.8.4)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (0.8.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (23.1)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (1.5.0)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (1.2.1)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->otter-grader) (2.18.0)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->otter-grader) (4.19.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->otter-grader) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->otter-grader) (2023.3.post1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->otter-grader) (1.23.5)\n","Requirement already satisfied: pydantic!=2.0.*,<3,>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-on-whales->otter-grader) (1.10.12)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from python-on-whales->otter-grader) (4.66.1)\n","Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from python-on-whales->otter-grader) (0.9.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-on-whales->otter-grader) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->otter-grader) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->otter-grader) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->otter-grader) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->otter-grader) (2023.7.22)\n","Requirement already satisfied: comm>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->otter-grader) (0.2.0)\n","Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->otter-grader) (1.6.6)\n","Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->otter-grader) (6.1.12)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->otter-grader) (1.5.7)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->otter-grader) (5.9.5)\n","Requirement already satisfied: pyzmq>=20 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->otter-grader) (23.2.1)\n","Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->otter-grader) (6.3.2)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->otter-grader) (0.8.3)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->otter-grader) (23.1.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->otter-grader) (2023.7.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->otter-grader) (0.30.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->otter-grader) (0.10.2)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->otter-grader) (3.10.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=1.0.0->jupytext->otter-grader) (0.1.2)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->otter-grader) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->otter-grader) (0.2.6)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (6.5.5)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->otter-grader) (2.5)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->otter-grader) (0.5.1)\n","Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (1.0.7)\n","Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (1.0.5)\n","Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (1.0.1)\n","Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (2.0.4)\n","Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (1.1.9)\n","Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (1.0.6)\n","Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (2.2.0)\n","Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (2.12.1)\n","Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (0.7.13)\n","Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (1.4.1)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (23.1.0)\n","Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.8.2)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (0.17.1)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (0.17.1)\n","Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.0.0)\n","Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.24.0)\n","Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (0.2.3)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (21.2.0)\n","Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (3.7.1)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.6.2)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.15.1)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.1.3)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (2.21)\n","Cloning into 'public'...\n","remote: Enumerating objects: 119, done.\u001b[K\n","remote: Counting objects: 100% (119/119), done.\u001b[K\n","remote: Compressing objects: 100% (74/74), done.\u001b[K\n","remote: Total 119 (delta 50), reused 100 (delta 34), pack-reused 0\u001b[K\n","Receiving objects: 100% (119/119), 1.03 MiB | 9.98 MiB/s, done.\n","Resolving deltas: 100% (50/50), done.\n"]}],"source":["!pip install otter-grader\n","!rm -rf public\n","!git clone https://github.com/chandar-lab/INF8250ae-assignments-2023 public"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"qgJuZk9qrlsw"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pygame in /root/.local/lib/python3.10/site-packages (2.5.2)\n","Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.42.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n"]}],"source":["!pip install -U pygame --user\n","!pip install gymnasium\n","!pip install matplotlib\n","!pip install tqdm"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"XOdLGqFuwZXd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n"]}],"source":["!apt-get install x11-utils > /dev/null 2>&1\n","!pip install pyglet > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n","!pip install pyvirtualdisplay > /dev/null 2>&1\n","!pip install pillow"]},{"cell_type":"markdown","metadata":{"id":"lLC3E0lgJosu"},"source":["### Importing Libraries"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"5NrZRkF3rlsx"},"outputs":[],"source":["import otter\n","grader = otter.Notebook(colab=True, tests_dir='./public/a4/tests')"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"LAeKW1awDf9f"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","import numpy as np\n","import torch\n","import torch.distributions as torchdist\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import warnings\n","import functools\n","import os\n","import matplotlib.pyplot as plt\n","from IPython import display as ipythondisplay\n","\n","import gymnasium as gym\n","from tqdm import tqdm\n","from PIL import Image\n","from IPython.display import Image as IPImage, display\n","\n","device = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n","warnings.filterwarnings('ignore')\n","torch.manual_seed(0)\n","np.random.seed(0)\n","import os"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"cGiAZ1fFrlsx"},"outputs":[],"source":["GRADESCOPE_ENV_VAR = \"RUNNING_IN_GRADESCOPE\"\n","\n","def running_in_gradescope():\n","    var = os.getenv(GRADESCOPE_ENV_VAR)\n","    if var is None:\n","        return False\n","    return var == 'yes'"]},{"cell_type":"markdown","metadata":{"id":"kV1l7_Q2EMuy"},"source":["# The Environment\n","\n","For this assignment, we will use  [`CartPole-v1`](https://gymnasium.farama.org/environments/classic_control/cart_pole/) from OpenAI Gymnasium. In this environment, the goal is to balance an inverted pendulum on a cart by moving the cart laterally. The state of the agent has four components:\n","\n","- The horizontal position of the cart, $x$\n","- The velocity of the cart, $\\dot{x}$\n","- The angle of the pendulum, measured relative to the vertical axis, $\\theta$\n","- The angular velocity of the pendulum, $\\dot\\theta$\n","\n","There are two actions in the action space:\n","- 0: push cart to the left\n","- 1: push cart to the right\n","\n","The agent receives a reward of $1$ at each timestep, and the episode ends when the pendulum drops too far ($|\\theta|$ is more than $12^o$) or when the cart goes out of bounds. Also, the environment truncates after 500 steps if it hasn't already terminated, so the greatest possible return is $200$."]},{"cell_type":"markdown","metadata":{"id":"Wn8e5oDkOIVQ"},"source":["# Part 0. Video Rendering\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"IxtNVy8Vv0h4"},"outputs":[],"source":["def render_video(env, policy = None, steps = 50):\n","    env.action_space.seed(0)\n","    obs, _ = env.reset(seed = 0)\n","    rewards = []\n","    image_list = []\n","    for i in range(steps):\n","        if policy == None:\n","            action = env.action_space.sample()\n","        else:\n","            action = policy.action(obs)\n","        obs, reward, terminated, truncated, info = env.step(action)\n","        rewards.append(reward)\n","        screen = env.render()\n","        image_list.append(screen)\n","\n","        done = terminated or truncated\n","        if done:\n","            print(\"Return: \", sum(rewards))\n","            break\n","        env.close()\n","    pil_images = [Image.fromarray(image) for image in image_list]\n","    pil_images[0].save(\"output.gif\", save_all=True, append_images=pil_images[1:], duration=50, loop=0)\n","    display(IPImage(\"output.gif\"))"]},{"cell_type":"markdown","metadata":{"id":"Le1UW7LkOUmK"},"source":["Now, let us see how a random policy performs."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"raHs7Jj79zw9"},"outputs":[{"name":"stderr","output_type":"stream","text":["error: XDG_RUNTIME_DIR not set in the environment.\n"]},{"name":"stdout","output_type":"stream","text":["Return:  18.0\n"]},{"data":{"image/gif":"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","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"}],"source":["env = gym.make('CartPole-v1', render_mode=\"rgb_array\", max_episode_steps=200)\n","render_video(env)"]},{"cell_type":"markdown","metadata":{"id":"b7GsoCAqk57n"},"source":["Notice that the random agent cannot balance the pendulum even for 20 time steps! In this assignment you will be implementing policy gradient algorithms to learn a better policy."]},{"cell_type":"markdown","metadata":{"id":"i-Y49oxALyvq"},"source":["# Part 1. Parameterized Policy Network (17 pts total)\n","\n","In this assignment, we will be studying Policy Gradient algorithms. In these algorithms, rather than using action-values to select actions, the policy itself is parameterized (in our case, by a neural network), and the policy is optimized directly via gradient ascent (although for practical purposes, we will be optimizing the negative of the objective via gradient descent).\n","\n","We will use a neural network to represent the policy here. The input to the neural network is a state, and the output should encode a probability distribution over the action space. Our environment has a discrete action space, so the policy should output parameters for a *Categorical distribution*.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1b-HE0MVAoGc"},"source":["## 1a: The Policy Network (5 pts)\n","As a first step, fill in the `policy_init_network` function, which should return a torch neural net that will be to produce policy distributions for input states. You are free to experiment with different neural network architectures later, but for this assignment we recommend the following. Using `torch.nn.Sequential`, make a multilayer perceptron (MLP) with the following layers:\n","\n","1. A linear layer of size `(state space dimension, 32)`, followed by a ReLU activation\n","1. A linear layer of size `(32, 32)`, followed by a ReLU activation\n","1. A linear layer of size `(32, number of actions)` followed by a Softmax activation, to make a probability distribution over actions."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"jFc7LOjmJejc","tags":[]},"outputs":[],"source":["def policy_init_network(env: gym.Env) -> nn.Module:\n","    # Your code here\n","    # ---------------------------------\n","    return torch.nn.Sequential(\n","        torch.nn.Linear(env.observation_space.shape[0], 32),\n","        torch.nn.ReLU(),\n","        torch.nn.Linear(32, 32),\n","        torch.nn.ReLU(),\n","        torch.nn.Linear(32, env.action_space.n),\n","        torch.nn.Softmax()\n","    )\n","    # ---------------------------------"]},{"cell_type":"code","execution_count":10,"metadata":{"deletable":false,"editable":false,"id":"scmE6dB2yrqw"},"outputs":[{"data":{"text/html":["<p><strong><pre style='display: inline;'>question 1a</pre></strong> passed! </p>"],"text/plain":["question 1a results: All test cases passed!"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["grader.check(\"question 1a\")"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"-esPol7gSH2l"},"source":["## 1b: The Policy Class (12 pts)\n","In this part, we will build a class to represent the parameterized policy. This will be done in a few steps. In the constructor of the `Policy` class, initialize the variable `opt`, which will be used to optimize the policy parameters. This variable is a `torch.optim.Optimizer`. The learning rate is $10^{-3}$.\n","\n","### Part I: Optimizer (2 pts)\n","Initialize the attribute `self.opt` to [Adam optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) with learning rate $10^{-3}$\n","### Part II: The policy distribution (5 pts)\n","Fill in the `dist` method. This method takes as input a state and outputs a torch [`Distribution`](https://pytorch.org/docs/stable/distributions.html) over actions.\n","\n","### Part III: Sampling actions (5 pts)\n","Fill in the `action` method. This method samples an action from the policy given a state."]},{"cell_type":"code","execution_count":94,"metadata":{"id":"jmTbUIJ-PYAz","tags":[]},"outputs":[],"source":["class Policy:\n","    def __init__(\n","        self, env: gym.Env, network: nn.Module, discount=0.99, name=\"Abstract Policy\"\n","    ):\n","        self.name = name\n","        self.network = network\n","        self.discount = discount\n","\n","        self.env = env\n","        self.obs_dim = env.observation_space.shape[0]\n","        self.n_actions = env.action_space.n\n","        # Your code here (Part I)\n","        # ========================\n","        self.opt = torch.optim.Adam(self.network.parameters(), lr=1e-3)\n","        # ========================\n","\n","    def distribution(self, x: np.ndarray) -> torchdist.distribution.Distribution:\n","        \"\"\"\n","        Get the distribution over actions for a given state\n","\n","        \"\"\"\n","        dist = None\n","\n","        # Your code here (Part II)\n","        # ========================\n","\n","        x = torch.Tensor(x).float()\n","        probs = self.network(x).unsqueeze(0)\n","\n","        dist = torchdist.Categorical(probs)\n","\n","        # ========================\n","        return dist\n","\n","    def action(self, x: np.ndarray) -> int:\n","        \"\"\"\n","        Sample an action from the policy at a given state\n","\n","        Input: a state encoded as a numpy array\n","        Output: an action encoded as an int\n","        \"\"\"\n","        action = None\n","        # Your code here (Part III)\n","        # ========================\n","        dist = self.distribution(x)\n","        action = dist.sample().item()\n","        # ========================\n","        return action\n","\n","    def update(self, states, actions, rewards, dones) -> float:\n","        raise NotImplementedError"]},{"cell_type":"code","execution_count":95,"metadata":{"deletable":false,"editable":false,"id":"khhAi41Lyrqw"},"outputs":[{"data":{"text/html":["<p><strong><pre style='display: inline;'>question 1b</pre></strong> passed! </p>"],"text/plain":["question 1b results: All test cases passed!"]},"execution_count":95,"metadata":{},"output_type":"execute_result"}],"source":["grader.check(\"question 1b\")"]},{"cell_type":"markdown","metadata":{"id":"uNy4XmeFrlsy"},"source":["## Generating Episode"]},{"cell_type":"markdown","metadata":{"id":"zdgVCpU8Savd"},"source":["Now, the following function rolls out an episode in the environment with the policy. The function should return `(states, actions, rewards, terminated, truncated)` where\n","\n","1. `states` is a record of the states observed over the course of the episodes.\n","1. `actions` is a record of the actions taken.\n","1. `rewards` is a record of the rewards earned.\n","1. `terminated` is an array of `bool`s that marks the termination of the episode.\n","1. `truncated` is an array of `bool`s that marks the truncation of the episode.\n","\n","Note that in this function, we do not append the final state."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"Cy1iw7lWSB4H"},"outputs":[],"source":["def generate_episode(env: gym.Env, policy: Policy):\n","    \"\"\"\n","    Generates an episode given an environment and policy\n","    Inputs:\n","        env - Gymnasium environment\n","        policy - policy for generating episode\n","    Returns\n","\n","    \"\"\"\n","    # Initialize lists\n","    states = []\n","    actions = []\n","    rewards = []\n","    terminated = []\n","    truncated = []\n","\n","    # Reset environment\n","    state, _ = env.reset(seed = 0)\n","    done = False\n","\n","    # Loop until end of episode\n","    while not done:\n","        states.append(state)\n","        # Get action\n","        action = policy.action(state)\n","        actions.append(action)\n","        # Take step\n","        state, reward, term, trunc,  _ = env.step(action)\n","        done = term or trunc\n","        rewards.append(reward)\n","        terminated.append(term)\n","        truncated.append(trunc)\n","    states = np.array(states)\n","    actions = np.array(actions)\n","    rewards = np.array(rewards)\n","    terminated = np.array(terminated)\n","    truncated = np.array(truncated)\n","    return (states, actions, rewards, terminated, truncated)"]},{"cell_type":"markdown","metadata":{"id":"hCQ_nxFsrlsy"},"source":["# Part 2. REINFORCE (17 pts total)\n","In this section, you will implement the REINFORCE algorithm."]},{"cell_type":"markdown","metadata":{"id":"jocPdu0gdDTs"},"source":["<!-- BEGIN QUESTION -->\n","\n","\n","## 2a: Discounting Rewards (10 pts total)\n","\n","This problem has 2 parts.\n","\n","Recall the form of the REINFORCE policy gradient:\n","\n","$$\n","\\nabla_\\theta J(\\theta) = \\sum_{k=0}^T\\mathbf{E}\\left\\{G^{\\pi_\\theta}\\nabla_\\theta\\log\\pi_\\theta(a_k\\mid s_k)\\right\\}\n","$$\n","\n","Here $\\pi_\\theta$ is the parameterized (neural net) policy with parameters $\\theta$, and $G^{\\pi_\\theta}$ is the random variable corresponding to the discounted return induced by following $\\pi_\\theta$. Note that at timestep $k$, action $a_k$ had no influence on rewards incurred before timestep $k$. For this reason, it is generally preferred to compute the following,\n","\n","$$\n","\\widehat\\nabla_\\theta J(\\theta) = \\sum_{k=0}^T\\mathbf{E}\\left\\{G^{\\pi_\\theta}_k\\nabla_\\theta\\log\\pi_\\theta(a_k\\mid s_k)\\right\\}\n","$$\n","\n","where\n","\n","$$\n","G^{\\pi_\\theta}_k = \\sum_{t=k}^T\\gamma^{t-k}r(s_k, a_k)\n","$$\n","\n","### Part I (5 pts):\n","**Question**: Why do you think it is preferred to substitute $\\nabla_\\theta J(\\theta)$ for $\\widehat\\nabla_\\theta J(\\theta)$ in policy gradient algorithms?"]},{"cell_type":"markdown","metadata":{"id":"htxLRto6rlsz"},"source":["_Type your answer here, replacing this text._"]},{"cell_type":"markdown","metadata":{"id":"Sdb2afgwhPbW"},"source":["<!-- END QUESTION -->\n","\n","### Part II (5 points):\n","Implement the function `discounted_returns`, which computes the values $(G_1^{\\pi_\\theta},G_2^{\\pi_\\theta},\\dots)$ for a given sequence of rewards.\n","\n","The function takes three arguments:\n","\n","1. `rewards`: An array of rewards, which may have been collected over several trajectories.\n","1. `dones`: An array of `bool`s, which mark where trajectories ended -- when either of `terminated` or `truncated` is `True`.\n","1. `discount`: The discount factor.\n","\n","The output of the function should be a list of the same length as `rewards` containing the cumulative discounted future returns starting at each step in the reward sequence. Mathematically, for some index $k$, if $T$ is the first index after $k$ for which `dones[T] = True`, then\n","\n","$$\n","\\texttt{returns}[k] = \\texttt{rewards}[k] + \\gamma\\texttt{rewards}[k+1] + \\dots + \\gamma^{T-k}\\texttt{rewards}[T]\\quad\n","$$\n","\n","For example, suppose we gather data from two trajectories, which had rewards `[1,2,3]` and `[4, 2, 1]` respectively. Then:\n","\n","- `rewards = [1,2,3,4,2,1]`\n","- `dones = [False, False, True, False, False, True]`\n","\n","For `discount = 0.5`, the output should be `[2.75, 3.5, 3, 5.25, 2.5, 1]`.\n","\n","**NOTE**: The output should be a numpy array."]},{"cell_type":"code","execution_count":82,"metadata":{"id":"Hoadj-naiOJk","tags":[]},"outputs":[],"source":["def discounted_returns(\n","    rewards: np.ndarray, dones: np.ndarray, discount: float\n",") -> np.ndarray:\n","    \"\"\"\n","    Compute discounted returns given rewards and terminateds\n","    Inputs:\n","        rewards - numpy array of reward values\n","        dones - numpy array consisting of boolean values for whether the episode has terminated.\n","        discount - discount factor\n","    Returns:\n","        returns - numpy array discounted returns\n","    \"\"\"\n","    # Your code here\n","    # ===============================\n","    returns = np.ndarray((len(rewards),), dtype=np.float64, buffer=np.array(rewards))\n","    for k in range(len(rewards) - 1, 0, -1):\n","        returns[k - 1] = rewards[k - 1] + discount * returns[k] * (1 - dones[k - 1])\n","    # ===============================\n","    return returns"]},{"cell_type":"code","execution_count":83,"metadata":{"deletable":false,"editable":false,"id":"x4HzO7Hhyrqx"},"outputs":[{"data":{"text/html":["<p><strong><pre style='display: inline;'>question 2a</pre></strong> passed! </p>"],"text/plain":["question 2a results: All test cases passed!"]},"execution_count":83,"metadata":{},"output_type":"execute_result"}],"source":["grader.check(\"question 2a\")"]},{"cell_type":"markdown","metadata":{"id":"0TbcxxeokvSp"},"source":["<!-- BEGIN QUESTION -->\n","\n","## 2b: The REINFORCE Update (7 pts)\n","Finally, we'll implement REINFORCE. Fill in the `update` method for the `REINFORCEPolicy` class below. This method takes the following inputs:\n","\n","1. `states`: An array of observed states.\n","1. `actions`: An array of actions taken at the corresponding `states`.\n","1. `rewards`: An array of rewards received, where `rewards[k]` is the reward for taking actions `actions[k]` at state `states[k]`.\n","1. `dones`: An array of `bool`s marking the end of episodes.\n","\n","This method should perform the following:\n","- Compute the average policy gradient \"loss\", which is $-\\sum_{n=1}^{T}G_n^{\\pi_\\theta}\\log\\pi_\\theta(a_n\\mid s_n)$,  averaged over all trajectories\n","- Compute the policy gradient\n","- Update the policy parameters\n","\n","The method should return a dictionary that contains information from the update. For now, the dictionary should only have one entry with key `'policy_loss'` that contains a scalar loss from the policy gradient computation."]},{"cell_type":"code","execution_count":126,"metadata":{"id":"fHIT-dd_k05l","tags":[]},"outputs":[],"source":["class REINFORCEPolicy(Policy):\n","    def __init__(\n","        self, env: gym.Env, network: nn.Module, discount=0.99, name=\"Plain REINFORCE\"\n","    ):\n","        super().__init__(env, network, discount=discount, name=name)\n","\n","    \"\"\"\n","    Perform a gradient update\n","    Inputs:\n","        states, actions, rewards, dones: Output from generate_episode function\n","    Returns:\n","        Dictionary with the following keys:\n","        - \"policy_loss\": float of the policy gradient loss (the quantity whose gradient is taken)\n","    \"\"\"\n","\n","    def update(self, states, actions, rewards, dones) -> dict:\n","        loss_dict = {}\n","        # Your code here\"\n","        # ======================\n","        G = discounted_returns(rewards, dones, self.discount)\n","        G = torch.Tensor(G)\n","        loss = -(G * self.distribution(states).log_prob(torch.Tensor(actions))).mean()  \n","\n","        self.opt.zero_grad()\n","        loss.backward()\n","        self.opt.step()\n","\n","        loss_dict[\"policy_loss\"] = loss.item()\n","        # ======================\n","        return loss_dict"]},{"cell_type":"code","execution_count":127,"metadata":{"deletable":false,"editable":false,"id":"c1LhJ0KUyrqx"},"outputs":[{"data":{"text/html":["<p><strong><pre style='display: inline;'>question 2b</pre></strong> passed! </p>"],"text/plain":["question 2b results: All test cases passed!"]},"execution_count":127,"metadata":{},"output_type":"execute_result"}],"source":["grader.check(\"question 2b\")"]},{"cell_type":"markdown","metadata":{"id":"sFJSefg4ig9k"},"source":["# Part 3. REINFORCE with Baseline (41 pts total)\n","When using a baseline in REINFORCE, the policy gradient formula is modified to the following,\n","\n","$$\n","\\nabla_\\theta J(\\theta) = \\sum_{k=0}^T\\mathbf{E}\\left\\{(G^{\\pi_\\theta} - b(s_k))\\nabla_\\theta\\log\\pi_\\theta(a_k\\mid s_k)\\right\\}\n","$$\n","\n","for some function $b:S\\to\\mathbf{R}$."]},{"cell_type":"markdown","metadata":{"id":"MsSzp_3zrls0"},"source":["<!-- BEGIN QUESTION -->\n","\n","## 3a: Understanding the baseline (12 pts)\n","1. **(3 pts)** What purpose does the baseline serve?\n","1. **(3 pts)** If the baseline is a constant (that is, $b(s_1) = b(s_2)$ for any pair of states $(s_1, s_2)$), should we expect the performance of REINFORCE with this baseline to be any different from standard REINFORCE?\n","1. **(3 pts)** Why can't the baseline be a function of the action as well as the state?\n","1. **(3 pts)** Does the inclusion of an arbitrary baseline always help?"]},{"cell_type":"markdown","metadata":{"id":"RQfBl2PYrls0"},"source":["_Type your answer here, replacing this text._"]},{"cell_type":"markdown","metadata":{"id":"E62H18RdnyMK"},"source":[]},{"cell_type":"markdown","metadata":{"id":"xLDPKzevkubU"},"source":["<!-- END QUESTION -->\n","\n","## 3b: The Value Function (5 pts)\n","In our experiments, we will use the value function as our baseline. It will be necessary to learn the value function from data, so our baseline will have the form\n","\n","$$\n","b(s) = V^{\\pi_\\theta}_\\phi(s)\n","$$\n","\n","where $\\phi$ denotes the parameters of the value function.\n","\n","Fill in the code for the construction of the value function neural net in `value_init_network`. The network architecture should be **similar** to that of the policy network besides the output layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wMOJYE4rktzZ","tags":[]},"outputs":[],"source":["def value_init_network(env: gym.Env) -> nn.Module:\n","    # Your code here\n","    # ===========================================================\n","    ...\n","    # ==========================================================="]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"xesiZkoayrq1"},"outputs":[],"source":["grader.check(\"question 3b\")"]},{"cell_type":"markdown","metadata":{"id":"3ZU1lnKerls6"},"source":["<!-- BEGIN QUESTION -->\n","\n","## 3c: REINFORCE with Baseline (9 pts)\n","\n","Fill in the constructor and the `update` method for `REINFORCEWithBaselinePolicy`.\n","\n","The constructor should do set two variables:\n","* `self.value_network`: the value function neural network\n","* `self.value_opt`: the `torch.optim.Optimizer` for the value function parameters. Use Adam optimizer and a learning rate of $2\\times 10^{-3}$ for the value optimizer.\n","\n","This method should perform the following:\n","- Compute the \"policy gradient loss\", using the value predictions from the value function network instead of the Monte Carlo return estimates\n","- Compute the policy gradient, again using the value predictions from the value function network instead of the Monte Carlo return estimates\n","- Update the policy parameters\n","- Compute the \"value loss\", which is mean squared difference between the Monte Carlo return estimates and the value function network predictions at each state in the trajectory\n","- Update the value function network parameters\n","\n","As with the standard REINFORCE case, the `update` method returns a dictionary with a key `'policy_loss'` reflecting the loss w.r.t. the policy gradient objective. For `REINFORCEWithBaselinePolicy`'s `update` method, however, the dictionary should also have a key `'value_loss'` reflecting the loss w.r.t. the value function error."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L5qlH1csrls6","tags":[]},"outputs":[],"source":["class REINFORCEWithBaselinePolicy(Policy):\n","    def __init__(self,\n","                env: gym.Env,\n","                policy_network: nn.Module,\n","                value_network: nn.Module,\n","                discount=0.99,\n","                name=\"REINFORCE with Baseline\"):\n","        super().__init__(env, policy_network, discount=discount, name=name)\n","        # Your code here\n","        ## Initialize value network and optimizer\n","        # ===========================================================\n","        ...\n","        # ===========================================================\n","\n","    \"\"\"\n","    Perform a gradient update\n","    Inputs:\n","        states, actions, rewards, dones: Output from rollout method\n","    Returns:\n","        Dictionary with the following keys:\n","        - \"policy_loss\": float of the policy gradient loss (the quantity whose gradient is taken)\n","        - \"value_loss\": float of the squared TD error\n","    \"\"\"\n","    def update(self, states, actions, rewards, dones) -> float:\n","        loss_dict = {}\n","        # Your code here\n","        # ======================\n","        ...\n","        # ======================\n","        return loss_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"CRU9mDNWyrq1"},"outputs":[],"source":["grader.check(\"question 3c\")"]},{"cell_type":"markdown","metadata":{"id":"me_n0O1yvdbm"},"source":["<!-- END QUESTION -->\n","\n","## 3d: Experiments (15 pts)\n","\n","The code below will train agents with REINFORCE with and without the value function baseline. Think about how you expect the return and loss curves to behave with and without the baseline."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ATEh_JdvUIS"},"outputs":[],"source":["env = gym.make('CartPole-v1', render_mode=\"rgb_array\", max_episode_steps=200)\n","agents = [\n","    REINFORCEPolicy(env, policy_init_network(env)),\n","    REINFORCEWithBaselinePolicy(env, policy_init_network(env), value_init_network(env))\n","]\n","\n","gradient_steps = 500\n","scores = [np.zeros(gradient_steps) for _ in agents]\n","stds = [np.zeros(gradient_steps) for _ in agents]\n","\n","test_runs = 5\n","\n","def rollout_score(env, policy):\n","    _, _, rewards, _, _ = generate_episode(env, policy)\n","    return np.sum(rewards)\n","\n","gs = list(range(gradient_steps))\n","\n","cmap = plt.get_cmap('viridis')\n","plt.figure()\n","fig, (ret_ax, loss_ax, value_ax) = plt.subplots(nrows=1, ncols=3, figsize=(18, 6))\n","\n","value_losses = []\n","\n","if not running_in_gradescope():\n","    for i in range(len(agents)):\n","        reinforce_policy = agents[i]\n","        print(f\"Training {reinforce_policy.name}\")\n","        losses = []\n","        for g in tqdm(range(gradient_steps)):\n","            states, actions, rewards, terminated, truncated = generate_episode(env, reinforce_policy)\n","            dones = [term or trunc for (term, trunc) in zip(terminated, truncated)]\n","            loss = reinforce_policy.update(states, actions, rewards, dones)\n","            losses.append(loss['policy_loss'])\n","            if 'value_loss' in loss.keys():\n","                value_losses.append(loss['value_loss'])\n","            res = [rollout_score(env, reinforce_policy) for _ in range(test_runs)]\n","            scores[i][g] = np.mean(res)\n","            stds[i][g] = np.std(res)\n","        color = cmap(i / len(agents))\n","        ret_ax.plot(gs, scores[i], label=reinforce_policy.name, color = color)\n","        ret_ax.fill_between(gs, scores[i] - stds[i], scores[i] + stds[i], alpha=0.3, color=color)\n","        loss_ax.plot(gs, losses, label=reinforce_policy.name, color = color)\n","    ret_ax.legend()\n","    ret_ax.grid(True)\n","    ret_ax.margins(0)\n","    ret_ax.set_title('Episode return')\n","    loss_ax.legend()\n","    loss_ax.grid(True)\n","    loss_ax.margins(0)\n","    loss_ax.set_title(\"Policy loss\")\n","    value_ax.plot(gs, value_losses, color = color)\n","    value_ax.grid(True)\n","    value_ax.margins(0)\n","    value_ax.set_title(\"Value loss\")\n","    plt.show()\n","plt.close('all')"]},{"cell_type":"markdown","metadata":{"id":"7K5HVAJOGVhW"},"source":["### Visualization"]},{"cell_type":"markdown","metadata":{"id":"krY6hNw6IdOP"},"source":["#### REINFORCE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-SNEBmOmGfSV"},"outputs":[],"source":["render_video(env, agents[0], steps = 200)"]},{"cell_type":"markdown","metadata":{"id":"_bFjh9S9Ih3e"},"source":["#### REINFORCE with Baseline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-BtqlNTUHbsL"},"outputs":[],"source":["render_video(env, agents[1], steps = 200)"]},{"cell_type":"markdown","metadata":{"id":"o4-9zM3Urls7"},"source":["### Analysis (5pts)\n","In your experiments, how did the use of the value function baseline affect your results? Explain the results you observed. Also, observe the visualizations above and qualitatively comment the nature of the policies obtained from the two agents (i.e., with and without baseline)."]},{"cell_type":"markdown","metadata":{"id":"HY0UTzB-rls7"},"source":["_Type your answer here, replacing this text._"]},{"cell_type":"markdown","metadata":{"id":"YhRgHXlL4fyH"},"source":["# 4. Actor-Critic (39 pts total)\n","\n","Finally, we will experiment with an *actor-critic* algorithm. Recall that the gradient rule for REINFORCE with the value function baseline has the following form,\n","\n","$$\n","\\nabla_\\theta J(\\theta) = \\sum_{k=0}^T\\mathbf{E}\\left\\{(G^{\\pi_\\theta} - V^{\\pi_\\theta}_\\phi(s_k))\\nabla_\\theta\\log\\pi_\\theta(a_k\\mid s_k)\\right\\}\n","$$\n","\n","Note that\n","\n","$$\n","\\mathbf{E}\\left\\{G^{\\pi_\\theta}\\mid s_0 = s\\right\\} = \\mathbf{E}_{a\\sim\\pi(\\cdot\\mid s),s'\\sim P(\\cdot\\mid s, a)}\\left\\{r(s, a) + \\gamma V^{\\pi_\\theta}(s')\\right\\}\n","$$\n","\n","Because of this, actor-critic algorithms estimate $G^{\\pi_\\theta}$ by $r(s, a) + \\gamma V^{\\pi_\\theta}(s')$. Thus, we can compute one gradient *per environment step*, since we no longer need data from the entire trajectory to estimate $G^{\\pi_\\theta}$. The gradient rule for the policy network (actor) is\n","\n","$$\n","\\nabla_\\theta J_{\\text{actor}}(\\theta) = \\mathbf{E}\\left\\{(r_k + \\gamma V^{\\pi_\\theta}_\\phi(s_{k+1}) - V^{\\pi_\\theta}_\\phi(s_k))\\nabla_\\theta\\log\\pi_\\theta(a_k\\mid s_k)\\right\\}\n","$$\n","\n","for the policy parameters. The value network (critic) is trained to minimize the mean squared TD error:\n","\n","$$\n","\\nabla_\\phi J_{\\text{critic}}(\\phi) = \\frac{1}{2}\\left(V^{\\pi_\\theta}_\\phi(s_k) - \\texttt{stop_gradient}\\left(r_k + \\gamma V^{\\pi_\\theta}_\\phi(s_{k+1})\\right)\\right)^2\n","$$\n","\n","where $\\texttt{stop_gradient}$ enforces that no gradients flow through its argument."]},{"cell_type":"markdown","metadata":{"id":"-goKflXerls7"},"source":["## 4a: Understanding Actor-Critic (15 pts total)\n","\n","This question is split into three conceptual questions."]},{"cell_type":"markdown","metadata":{"id":"Mjrf55mBrls7"},"source":["<!-- BEGIN QUESTION -->\n","\n","### Part I: Bias in Actor-Critic (5 pts)\n","It is said that actor-critic policy gradients are more biased than REINFORCE policy gradients. Explain what this means. Are actor-critic policy gradients more biased than REINFORCE policy gradients computed with the value function baseline?"]},{"cell_type":"markdown","metadata":{"id":"WEQdfZ2xrls7"},"source":["_Type your answer here, replacing this text._"]},{"cell_type":"markdown","metadata":{"id":"f3OUUvUKrls7"},"source":["<!-- END QUESTION -->\n","\n","<!-- BEGIN QUESTION -->\n","\n","### Part II: Per-Step Updates (5 pts)\n","Even though actor-critic algorithms can perform one update per step, the gradients are computed based on data from only one state transition as opposed to REINFORCE gradients which are averaged over $T$ state transitions. What is the benefit of updating once per environment step?"]},{"cell_type":"markdown","metadata":{"id":"9A6igDiarls7"},"source":["_Type your answer here, replacing this text._"]},{"cell_type":"markdown","metadata":{"id":"9rqlImbErls7"},"source":["<!-- END QUESTION -->\n","\n","<!-- BEGIN QUESTION -->\n","\n","### Part III: Lifelong Learning (5 pts)\n","Imagine a scenario where an RL agent is to be deployed on a strange planet that we do not know how to simulate. Once we drop the robot on this planet, we can never interact with it again: it just autonomously learns from environment interactions for the rest of its life. Would you prefer to employ Actor-Critic or REINFORCE with baseline for this problem? Why?"]},{"cell_type":"markdown","metadata":{"id":"z4-Y-i68rls7"},"source":["_Type your answer here, replacing this text._"]},{"cell_type":"markdown","metadata":{"id":"vBYhN3uU9CDk"},"source":["<!-- END QUESTION -->\n","\n","<!-- BEGIN QUESTION -->\n","\n","## 4b: Implementing Actor-Critic (9 pts)\n","Fill out the `ActorCriticPolicy` class below, according to the guidelines in the code. The `policy_init_network` and `value_init_network` methods will be used to instantiate the neural nets for the actor-critic, however they are trained differently in the actor-critic algorithm.\n","\n","Rather than implementing an `update` method for actor-critic, we will implement a method `train_episode` which rolls out an episode, performing updates at each step. More precisely, `train_episode` should do the following:\n","\n","1. Reset the environment to a starting state\n","1. For each environment step:\n","    1. Choose an action\n","    1. Perform an environment step with the chosen action, observing the next state, reward, and terminal signal\n","    1. Update both the actor and critic networks based on this transition\n","1. Return a dictionary with the same entries as `REINFORCEWithBaselinePolicy`'s `update` method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ObWy7-gv9BRf","tags":[]},"outputs":[],"source":["class ActorCriticPolicy(Policy):\n","    def __init__(self,\n","                env: gym.Env,\n","                policy_network: nn.Module,\n","                value_network: nn.Module,\n","                discount=0.99,\n","                name=\"Actor-Critic\"):\n","        super().__init__(env, policy_network, discount=discount, name=name)\n","        # Your code here\n","        # Initialize self.value_network and self.value_opt like before\n","        # ===========================================================\n","        ...\n","        # ===========================================================\n","\n","    \"\"\"\n","    Run a training episode\n","    Inputs:\n","        seed: Seed of the environment (default: 0)\n","    Returns:\n","        Dictionary with the following keys:\n","        - \"policy_loss\": float of the policy gradient loss (the quantity whose gradient is taken),\n","                        averaged over the episode\n","        - \"value_loss\": float of the squared TD error averaged over the episode\n","    \"\"\"\n","    def train_episode(self, seed = 0) -> float:\n","        loss_dict = {}\n","        state, _ = self.env.reset(seed = seed)\n","        # Your code here\n","        # ======================\n","        ...\n","        # ======================\n","        return loss_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"2ZpB1DcQyrq2"},"outputs":[],"source":["grader.check(\"question 4b\")"]},{"cell_type":"markdown","metadata":{"id":"8U41kVqmBKHC"},"source":["<!-- END QUESTION -->\n","\n","## 4c: Experiments (15 pts)\n","\n","In the following experiments, we test the following agents:\n","\n","- REINFORCE with one trajectory per gradient update\n","- REINFORCE with the value function baseline, one trajectory per gradient update\n","- Actor-Critic\n","\n","Each agent is trained for 400 episodes, and the experiment is repeated 6 times with different random seeds. The plot displays the mean and variance of the return across the seeds for each agent."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i8cqS3dZrls8"},"outputs":[],"source":["env = gym.make('CartPole-v1', render_mode=\"rgb_array\", max_episode_steps=200)\n","import itertools\n","SEEDS = [4, 8, 16, 23, 42]\n","\n","episodes = 500\n","eval_runs = 5\n","eval_every = 5\n","epochs = list(range(0, episodes, eval_every))\n","\n","cmap = plt.get_cmap('viridis')\n","plt.grid(True)\n","plt.margins(0)\n","plt.xlabel(\"Episode\")\n","plt.ylabel(\"Return\")\n","\n","pg_constructor = lambda: REINFORCEPolicy(env, policy_init_network(env))\n","pg_baseline_constructor = lambda: REINFORCEWithBaselinePolicy(env, policy_init_network(env), value_init_network(env))\n","\n","ac_agent_constructor = lambda: ActorCriticPolicy(env, policy_init_network(env), value_init_network(env))\n","\n","if not running_in_gradescope():\n","    ### ACTOR CRITIC\n","    ac_agents = {}\n","    ac_score_traces = {}\n","    print(f\"Training Actor-Critic\")\n","    for (seed, ep) in tqdm(itertools.product(SEEDS, np.arange(episodes))):\n","        if seed not in ac_agents.keys():\n","            np.random.seed(seed)\n","            torch.manual_seed(seed)\n","            ac_agents[seed] = ac_agent_constructor()\n","            ac_score_traces[seed] = []\n","            ac_agents[seed].env.action_space.seed(seed)\n","        agent = ac_agents[seed]\n","        loss = agent.train_episode()\n","        if (ep + 1) % eval_every == 0:\n","            res = [rollout_score(env, agent) for _ in range(eval_runs)]\n","            ac_score_traces[seed].append(np.mean(res))\n","\n","    ac_data = np.vstack([ac_score_traces[seed] for seed in SEEDS])\n","    ac_score_mean = np.mean(ac_data, axis=0)\n","    ac_score_std = np.std(ac_data, axis=0)\n","\n","    plt.plot(epochs, ac_score_mean, color=cmap(0.8), label='Actor Critic')\n","    plt.fill_between(\n","        epochs,\n","        ac_score_mean - ac_score_std,\n","        ac_score_mean + ac_score_std,\n","        color=cmap(0.8),\n","        alpha=0.3\n","    )\n","\n","    ### REINFORCE WITH BASELINE\n","    pg_baseline_agents = {}\n","    pg_baseline_score_traces = {}\n","    print(f\"Training REINFORCE with Baseline\")\n","    for (seed, ep) in tqdm(itertools.product(SEEDS, np.arange(episodes))):\n","        if seed not in pg_baseline_agents.keys():\n","            np.random.seed(seed)\n","            torch.manual_seed(seed)\n","            pg_baseline_agents[seed] = pg_baseline_constructor()\n","            pg_baseline_score_traces[seed] = []\n","            env.env.action_space.seed(seed)\n","        agent = pg_baseline_agents[seed]\n","        states, actions, rewards, terminated, truncated = generate_episode(env, agent)\n","        dones = [term or trunc for (term, trunc) in zip(terminated, truncated)]\n","        loss = agent.update(states, actions, rewards, dones)\n","        if (ep + 1) % eval_every == 0:\n","            res = [rollout_score(env, agent) for _ in range(eval_runs)]\n","            pg_baseline_score_traces[seed].append(np.mean(res))\n","\n","    pg_baseline_data = np.vstack([pg_baseline_score_traces[seed] for seed in SEEDS])\n","    pg_baseline_score_mean = np.mean(pg_baseline_data, axis=0)\n","    pg_baseline_score_std = np.std(pg_baseline_data, axis=0)\n","\n","    plt.plot(epochs, pg_baseline_score_mean, color=cmap(0.5), label='REINFORCE with Baseline')\n","    plt.fill_between(\n","        epochs,\n","        pg_baseline_score_mean - pg_baseline_score_std,\n","        pg_baseline_score_mean + pg_baseline_score_std,\n","        color=cmap(0.5),\n","        alpha=0.3\n","    )\n","\n","    ### REINFORCE\n","    pg_agents = {}\n","    pg_score_traces = {}\n","    print(f\"Training REINFORCE with Baseline\")\n","    for (seed, ep) in tqdm(itertools.product(SEEDS, np.arange(episodes))):\n","        if seed not in pg_agents.keys():\n","            np.random.seed(seed)\n","            torch.manual_seed(seed)\n","            pg_agents[seed] = pg_constructor()\n","            pg_score_traces[seed] = []\n","            env.env.action_space.seed(seed)\n","        agent = pg_agents[seed]\n","        states, actions, rewards, terminated, truncated = generate_episode(env, agent)\n","        dones = [term or trunc for (term, trunc) in zip(terminated, truncated)]\n","        loss = agent.update(states, actions, rewards, dones)\n","        if (ep + 1) % eval_every == 0:\n","            res = [rollout_score(env, agent) for _ in range(eval_runs)]\n","            pg_score_traces[seed].append(np.mean(res))\n","\n","    pg_data = np.vstack([pg_score_traces[seed] for seed in SEEDS])\n","    pg_score_mean = np.mean(pg_data, axis=0)\n","    pg_score_std = np.std(pg_data, axis=0)\n","\n","    plt.plot(epochs, pg_score_mean, color=cmap(0.2), label='REINFORCE')\n","    plt.fill_between(\n","        epochs,\n","        pg_score_mean - pg_score_std,\n","        pg_score_mean + pg_score_std,\n","        color=cmap(0.2),\n","        alpha=0.3\n","    )\n","\n","    plt.legend()"]},{"cell_type":"markdown","metadata":{"id":"smA1VXUraGMu"},"source":["### Visualizing AC policy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HPpZH-NuZmpx"},"outputs":[],"source":["env = gym.make('CartPole-v1', render_mode=\"rgb_array\", max_episode_steps=200)\n","render_video(env, policy = ac_agents[4], steps = 200)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w8ROAYOprls8"},"outputs":[],"source":["plt.close('all')"]},{"cell_type":"markdown","metadata":{"id":"ISzgFQb9rls8"},"source":["### Analysis (5 pts)\n","Based on your experiments, does actor-critic perform favorably to REINFORCE (with and/or without baseline)? Explain your observations based on the learning curves and visualization."]},{"cell_type":"markdown","metadata":{"id":"qP4OGSWwrls8"},"source":["_Type your answer here, replacing this text._"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"otter":{"OK_FORMAT":true,"tests":{"question 1a":{"name":"question 1a","points":5,"suites":[{"cases":[{"code":">>> test_env = gym.make('CartPole-v0')\n>>> test_output = policy_init_network(test_env)\n>>> test_state = torch.tensor([-0.04, 0.02, -0.04, 0.02])\n>>> test_out = test_output(test_state)\n>>> np.testing.assert_allclose(test_out.shape[0], 2)\n>>> np.testing.assert_allclose(test_out.detach().numpy().sum(), 1.0, rtol=1e-5, atol=0)\n","hidden":false,"locked":false}],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"question 1b":{"name":"question 1b","points":[5,5,5],"suites":[{"cases":[{"code":">>> test_state = env.reset()\n>>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_policy = Policy(env, policy_init_network(env), 0.99)\n>>> assert_equal(isinstance(test_policy.opt, torch.optim.Optimizer), True)\n","hidden":false,"locked":false},{"code":">>> test_state = env.reset()\n>>> test_policy = Policy(env, policy_init_network(env), 0.99)\n>>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> assert_equal(isinstance(test_policy.dist(test_state).param_shape, torch.Size), True)\n>>> assert_equal(test_policy.dist(test_state).param_shape, torch.Size([1,2]))\n","hidden":false,"locked":false},{"code":">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_state = env.reset()\n>>> test_policy = Policy(env, policy_init_network(env), 0.99)\n>>> assert_equal(isinstance(test_policy.dist(test_state), torch.distributions.Categorical), True)\n>>> assert_equal(test_policy.action(test_state) in [0,1], True)\n","hidden":true,"locked":false}],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"question 1c":{"name":"question 1c","points":[3,3,4],"suites":[{"cases":[{"code":">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_env = gym.make('CartPole-v0')\n>>> test_policy = Policy(test_env, policy_init_network(test_env), 0.99)\n>>> num_rollouts = 3\n>>> test_s, test_a, test_r, test_d = rollout(test_env, test_policy, num_rollouts)\n>>> test_len_rollout = test_s.shape[0]\n>>> assert_equal(isinstance(test_s, np.ndarray), True)\n>>> assert_equal(isinstance(test_a, np.ndarray), True)\n>>> assert_equal(isinstance(test_r, np.ndarray), True)\n>>> assert_equal(isinstance(test_d, np.ndarray), True)\n","hidden":false,"locked":false},{"code":">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_env = gym.make('CartPole-v0')\n>>> test_policy = Policy(test_env, policy_init_network(test_env), 0.99)\n>>> num_rollouts = 3\n>>> test_s, test_a, test_r, test_d = rollout(test_env, test_policy, num_rollouts)\n>>> test_len_rollout = test_s.shape[0]\n>>> assert_equal(test_s.shape, (test_len_rollout, 4))\n>>> assert_equal(test_a.shape, (test_len_rollout,))\n>>> assert_equal(test_r.shape, (test_len_rollout,))\n>>> assert_equal(test_d.shape, (test_len_rollout,))\n","hidden":false,"locked":false},{"code":">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_env = gym.make('CartPole-v0')\n>>> test_policy = Policy(test_env, policy_init_network(test_env), 0.99)\n>>> num_rollouts = 3\n>>> test_s, test_a, test_r, test_d = rollout(test_env, test_policy, num_rollouts)\n>>> assert_equal(np.any((test_a != 0)&(test_a != 1 )), False)\n>>> assert_equal(np.sum(test_d), num_rollouts)\n","hidden":true,"locked":false}],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"question 2a.3":{"name":"question 2a.3","points":[2,2,2,3,3],"suites":[{"cases":[{"code":">>> test_len_rollout = 5\n>>> test_d = np.array([True for _ in range(test_len_rollout)])\n>>> test_r = np.ones(test_len_rollout)\n>>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_ret = discounted_returns(test_r, test_d, 0.99)\n>>> assert_equal(test_ret.shape[0], test_len_rollout)\n","hidden":false,"locked":false},{"code":">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=1e-5, atol=0)\n>>> test_rewards = [1.0,2.0,3.0,4.0,2.0,1.0]\n>>> test_dones = [False, False, True, False, False, True]\n>>> test_return = discounted_returns(test_rewards, test_dones, 0.5)\n>>> actual_return = np.array([2.75, 3.5, 3.0, 5.25, 2.5, 1.0])\n>>> assert_equal(test_return, actual_return)\n","hidden":false,"locked":false},{"code":">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=1e-5, atol=0)\n>>> test_rewards = [1.0,2.0,3.0,4.0,2.0,1.0]\n>>> test_dones = [False, False, True, False, False, True]\n>>> test_return_perpetuity = discounted_returns(test_rewards, test_dones, 0.5, perpetuity=True)\n>>> actual_return_perpetuity = np.array([3.5, 5., 6., 5.5, 3., 2.])\n>>> assert_equal(test_return_perpetuity, actual_return_perpetuity)\n","hidden":false,"locked":false},{"code":">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=1e-5, atol=0)\n>>> _other_discount = 0.8\n>>> _other_rewards = [2.0, 3.0, 1.0, 4.0, 5.0, 6.0]\n>>> _other_dones = [False, True, False, True, False, True]\n>>> _other_returns_true = np.array([4.4, 3.0, 4.2, 4.0, 9.8, 6.0])\n>>> _other_returns_pred = discounted_returns(_other_rewards, _other_dones, _other_discount)\n>>> assert_equal(_other_returns_pred, _other_returns_true)\n","hidden":true,"locked":false},{"code":">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=1e-5, atol=0)\n>>> _other_discount = 0.8\n>>> _other_rewards = [2.0, 3.0, 1.0, 4.0, 5.0, 6.0]\n>>> _other_dones = [False, True, False, True, False, True]\n>>> _other_returns_perp_true = np.array([14., 15., 17., 20., 29., 30.])\n>>> _other_returns_perp_pred = discounted_returns(_other_rewards, _other_dones, _other_discount, perpetuity=True)\n>>> assert_equal(_other_returns_perp_pred, _other_returns_perp_true)\n","hidden":true,"locked":false}],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"question 2b":{"name":"question 2b","points":2,"suites":[{"cases":[{"code":">>> test_env = gym.make('CartPole-v0')\n>>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_reinforce = REINFORCEPolicy(test_env, policy_init_network(test_env), 0.99)\n>>> test_states, test_actions, test_rewards, test_dones = rollout(test_env, test_reinforce, 2)\n>>> loss = test_reinforce.update(test_states, test_actions, test_rewards, test_dones)\n>>> assert_equal(isinstance(loss,dict), True)\n>>> assert_equal('policy_loss' in loss, True)\n>>> assert_equal(isinstance(loss['policy_loss'], float), True)\n","hidden":false,"locked":false}],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"question 3b":{"name":"question 3b","points":3,"suites":[{"cases":[{"code":">>> test_env = gym.make('CartPole-v0')\n>>> test_state = test_env.reset()\n>>> test_value_net = value_init_network(test_env)\n>>> test_value = test_value_net(torch.FloatTensor(test_state))\n>>> np.testing.assert_allclose(test_value.shape[0], 1)\n","hidden":false,"locked":false}],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"question 3c":{"name":"question 3c","points":[1,1,1],"suites":[{"cases":[{"code":">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_env = gym.make('CartPole-v0')\n>>> test_value_net = value_init_network(test_env)\n>>> test_state = torch.tensor([-0.04, 0.02, -0.04, 0.02])\n>>> test_reinforce_baseline = REINFORCEWithBaselinePolicy(test_env, policy_init_network(test_env), test_value_net,0.99)\n>>> assert_equal(isinstance(test_reinforce_baseline.value_opt, torch.optim.Optimizer), True)\n","hidden":false,"locked":false},{"code":">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_env = gym.make('CartPole-v0')\n>>> test_value_net = value_init_network(test_env)\n>>> test_state = torch.tensor([-0.04, 0.02, -0.04, 0.02])\n>>> test_reinforce_baseline = REINFORCEWithBaselinePolicy(test_env, policy_init_network(test_env), test_value_net,0.99)\n>>> test_states, test_actions, test_rewards, test_dones = rollout(test_env, test_reinforce_baseline, 2)\n>>> loss = test_reinforce_baseline.update(test_states, test_actions, test_rewards, test_dones)\n>>> assert_equal(isinstance(loss,dict), True)\n>>> assert_equal('policy_loss' in loss, True)\n","hidden":false,"locked":false},{"code":">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_env = gym.make('CartPole-v0')\n>>> test_value_net = value_init_network(test_env)\n>>> test_state = torch.tensor([-0.04, 0.02, -0.04, 0.02])\n>>> test_reinforce_baseline = REINFORCEWithBaselinePolicy(test_env, policy_init_network(test_env), test_value_net,0.99)\n>>> test_states, test_actions, test_rewards, test_dones = rollout(test_env, test_reinforce_baseline, 2)\n>>> loss = test_reinforce_baseline.update(test_states, test_actions, test_rewards, test_dones)\n>>> assert_equal('value_loss' in loss, True)\n","hidden":false,"locked":false}],"scored":true,"setup":"","teardown":"","type":"doctest"}]},"question 4b":{"name":"question 4b","points":[2,3],"suites":[{"cases":[{"code":">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_env = gym.make('CartPole-v0')\n>>> test_value_net = value_init_network(test_env)\n>>> test_actor_critic = ActorCriticPolicy(test_env, policy_init_network(test_env), test_value_net,0.99)\n>>> loss = test_actor_critic.train_episode()\n","hidden":false,"locked":false},{"code":">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_env = gym.make('CartPole-v0')\n>>> test_value_net = value_init_network(test_env)\n>>> test_actor_critic = ActorCriticPolicy(test_env, policy_init_network(test_env), test_value_net,0.99)\n>>> loss = test_actor_critic.train_episode()\n>>> assert_equal(isinstance(loss,dict), True)\n>>> assert_equal('policy_loss' in loss, True)\n>>> assert_equal('value_loss' in loss, True)\n","hidden":false,"locked":false}],"scored":true,"setup":"","teardown":"","type":"doctest"}]}}}},"nbformat":4,"nbformat_minor":0}
