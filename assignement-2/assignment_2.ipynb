{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/Depdx/INF8250AE--Reinforcement-Learning/blob/main/assignement-2/assignement_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx--OudV4QVT"
      },
      "source": [
        "# Assignment 2: Imitation Learning\n",
        "In this assignment, you will implement the basic components of an Imitation Learning\n",
        "system, behavior cloning, and DAgger.\n",
        "## Instructions\n",
        "- This is an individual assignment. You are not allowed to discuss the problems with other students.\n",
        "- Part of this assignment will be autograded by gradescope. You can use it as immediate feedback to improve your answers. You can resubmit as many times as you want.\n",
        "- All your solution, code, analysis, graphs, explanations should be done in this same notebook.\n",
        "- Please make sure to execute all the cells before you submit the notebook to the gradescope. - You will not get points for the plots if they are not generated already.\n",
        "- If you have questions regarding the assignment, you can ask for clarifications in\n",
        "  Piazza. You should use the corresponding tag for this assignment.\n",
        "- Start Early! Some of the cells can take about an hour to run on CPU. You will need\n",
        "  time to generate the results.\n",
        "  \n",
        "**When Submitting to GradeScope**: Be sure to\n",
        "1. Submit a .ipynb notebook to the Assignment 2 - Code section on Gradescope.\n",
        "2. Submit a pdf version of the notebook to the Assignment 2 - Report entry.\n",
        "\n",
        "Note: You can choose to submit responses in either English or French.\n",
        "\n",
        "Before starting the assignment, make sure that you have downloaded all the tests related\n",
        "for the assignment and put them in the appropriate locations. If you run the next cell,\n",
        "we will set this all up automatically for you in a dataset called public, which will\n",
        "contain both the data and tests you use.\n",
        "\n",
        "This assignment has 4 questions. You will learn to:\n",
        "1. Implement basic components in an Imitation Learning/RL setup.\n",
        "2. Implement behavior cloning.\n",
        "3. Implement DAgger.\n",
        "4. Analyze different aspects of the DAgger algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49FfhdId4QVU",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "!apt update\n",
        "!apt install -y --no-install-recommends \\\n",
        "        build-essential \\\n",
        "        curl \\\n",
        "        git \\\n",
        "        gnupg2 \\\n",
        "        make \\\n",
        "        cmake \\\n",
        "        ffmpeg \\\n",
        "        swig \\\n",
        "        libz-dev \\\n",
        "        unzip \\\n",
        "        zlib1g-dev \\\n",
        "        libglfw3 \\\n",
        "        libglfw3-dev \\\n",
        "        libxrandr2 \\\n",
        "        libxinerama-dev \\\n",
        "        libxi6 \\\n",
        "        libxcursor-dev \\\n",
        "        libgl1-mesa-dev \\\n",
        "        libgl1-mesa-glx \\\n",
        "        libglew-dev \\\n",
        "        libosmesa6-dev \\\n",
        "        lsb-release \\\n",
        "        ack-grep \\\n",
        "        patchelf \\\n",
        "        wget \\\n",
        "        xpra \\\n",
        "        xserver-xorg-dev \\\n",
        "        ffmpeg\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGkhWwWt4QVV",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium[mujoco]\n",
        "!pip install torch\n",
        "!pip install tqdm\n",
        "!pip install matplotlib\n",
        "!pip install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tC9PXCeD4QVV"
      },
      "outputs": [],
      "source": [
        "!pip install otter-grader\n",
        "!git clone https://github.com/chandar-lab/INF8250ae-assignments-2023.git public"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roxsb0-C4QVV"
      },
      "outputs": [],
      "source": [
        "#@title set up virtual display\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BabfQXQv4QVV"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import wrappers\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import os\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6_cQz0p4QVV",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import otter\n",
        "grader = otter.Notebook(colab=True, tests_dir='./public/a2/tests')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wf-jc3rk4QVV"
      },
      "outputs": [],
      "source": [
        "def plot(\n",
        "    xs_list,\n",
        "    means_list,\n",
        "    stds_list,\n",
        "    losses_list,\n",
        "    labels_list=None,\n",
        "    min=None,\n",
        "    running_average=5,\n",
        "):\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    if labels_list is None:\n",
        "        labels_list = [f\"Agent {idx}\" for idx in range(len(means_list))]\n",
        "    for xs, means, stds, losses, label in zip(\n",
        "        xs_list, means_list, stds_list, losses_list, labels_list\n",
        "    ):\n",
        "        kernel = np.ones(running_average) / running_average\n",
        "        means_convolved = np.convolve(means, kernel, mode=\"same\")\n",
        "        stds_convolved = np.convolve(stds, kernel, mode=\"same\")\n",
        "        ax[0].plot(xs, means_convolved, label=label)\n",
        "        ax[0].fill_between(\n",
        "            xs,\n",
        "            np.array(means_convolved) - np.array(stds_convolved),\n",
        "            np.array(means_convolved) + np.array(stds_convolved),\n",
        "            alpha=0.5,\n",
        "        )\n",
        "        ax[1].plot(xs, losses, label=label)\n",
        "    if min is not None:\n",
        "        ax[0].set_ylim(min, None)\n",
        "    ax[0].legend()\n",
        "    ax[0].set_ylabel(\"Reward\")\n",
        "    ax[1].set_ylabel(\"Loss\")\n",
        "\n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vml_nyOL4QVV"
      },
      "outputs": [],
      "source": [
        "class ExpertAgent(torch.nn.Module):\n",
        "    def __init__(self, filename):\n",
        "        super().__init__()\n",
        "        self._network = torch.load(filename)\n",
        "        self._network.eval()\n",
        "\n",
        "    def get_action(self, obs: np.array):\n",
        "        \"\"\"\n",
        "        Get action from the expert agent.\n",
        "\n",
        "        Args:\n",
        "            obs: np.array of shape (state_dim,)\n",
        "        Returns:\n",
        "            action: np.array of shape (action_dim,)\n",
        "        \"\"\"\n",
        "        obs = torch.tensor(obs, dtype=torch.float32)\n",
        "        return self._network(obs).cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNCrH_sC4QVV"
      },
      "source": [
        "# Q1 Getting started with RL (10 pts)\n",
        "For this assignment, we will be using the [Ant-v4](https://gymnasium.farama.org/environments/mujoco/ant/) environment. The goal in this environment is to have the \"Ant\" run as far as it can for 1000\n",
        "timesteps, with the reward being a linear combination of how far it ran, how long it was\n",
        "in a \"healthy\" state, and a penalty for taking actions  that are too large. The actions\n",
        "control the torque for the motors at each of the 8 joints of the agent.\n",
        "\n",
        "This environment is part of the [gymnasium](https://gymnasium.farama.org/) package, a\n",
        "library which provides a standard interface for environments used across many different\n",
        "RL research projects. For this assignment, you will need to familiar with the interface\n",
        "provided by the [Env](https://gymnasium.farama.org/api/env/) class. Specifically,\n",
        "`env.reset()` and `env.step()`. `env.reset()` resets the environment and agent to the\n",
        "start of the episode. It does not have any required arguments, and it returns `(obs, info)`,\n",
        "where `obs` is the first observation of the episode, and `info` is a dictionary\n",
        "containing additional information (you will not need to interact with `info`). To\n",
        "take actions in the environment, call `env.step`, which takes in an action, and returns\n",
        "`(obs, reward, terminated, truncated, info)`, where `obs` is the next state, `reward` is\n",
        "the reward for step just taken, `terminated` refers to whether the episode entered a\n",
        "terminal state, `truncated` refers to whether the episode was ended before entering a\n",
        "terminal state, and `info` contains any extra info the environment wants to provide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "siHzBV8b4QVV"
      },
      "source": [
        "## Q1.a: Agent Evaluation (4 pts)\n",
        "As a warmup and introduction to interactive environments, implement the `evaluate_agent`\n",
        "function below. It should collect `num_episodes` trajectories in the environment, and\n",
        "return the mean and standard deviation of the episode returns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "m_jYoBj-4QVW",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def evaluate_agent(agent, env:gym.Env, num_episodes:int):\n",
        "    \"\"\" Collect num_episodes trajectories for the agent and compute mean and std of the\n",
        "    rewards. Remember to reset the environment before each episode.\n",
        "    Args:\n",
        "        agent: Agent, agent to evaluate\n",
        "        env: gym.Env, environment to evaluate agent on\n",
        "        num_episodes: int, number of episodes to evaluate the agent for\n",
        "    Returns:\n",
        "        mean_return: float, mean return over the episodes\n",
        "        std_return: float, standard deviation of the return over the episodes\n",
        "    \"\"\"\n",
        "    returns = []\n",
        "    for _ in tqdm(range(num_episodes), desc=\"Evaluating agent\", position=1, leave=False):\n",
        "        obs, info = env.reset()\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        total_reward = 0\n",
        "        while not terminated and not truncated:\n",
        "            action = agent.get_action(obs)\n",
        "            obs, reward, terminated, truncated, info = env.step(action)\n",
        "            total_reward += reward\n",
        "        returns.append(total_reward)\n",
        "    mean_return = np.mean(returns)\n",
        "    std_return = np.std(returns)\n",
        "    return mean_return, std_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "t5fZSuoD4QVW"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q1a\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCjDny-C4QVW"
      },
      "outputs": [],
      "source": [
        "VIDEO_LOCATION = \"./content/video\"\n",
        "\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob(f\"{VIDEO_LOCATION}/*.mp4\")\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, \"r+b\").read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(\n",
        "            HTML(\n",
        "                data=\"\"\"<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>\"\"\".format(\n",
        "                    encoded.decode(\"ascii\")\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def create_video(vis_env, agent, name_prefix=\"imitation_learning\"):\n",
        "    vis_env = wrappers.RecordVideo(vis_env, VIDEO_LOCATION, name_prefix=name_prefix)\n",
        "    evaluate_agent(agent, vis_env, 1)\n",
        "    vis_env.close_video_recorder()\n",
        "    show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq-8LLn74QVW"
      },
      "source": [
        "Let's now visualize what this looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-d1JNjJ4QVW",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Ant-v4\")\n",
        "vis_env = gym.make(\"Ant-v4\", render_mode=\"rgb_array\")\n",
        "a = env.action_space\n",
        "expert_1mil = ExpertAgent(\"./public/a2/experts/network_1mil.pt\")\n",
        "mean, std = evaluate_agent(expert_1mil, env, 10)\n",
        "print(f\"Expert mean return: {mean} +/- {std}\")\n",
        "create_video(vis_env, expert_1mil, \"expert_1mil\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "5IT_ojCr4QVW"
      },
      "source": [
        "## Q1.b: Replay Buffer (3 pts)\n",
        "Next, we will implement a replay buffer. In RL, we typically store states, actions,\n",
        "rewards, next states, and termination for each transition, but for this assignment,\n",
        "because we are only doing imitation learning (not learning from rewards!), we only need\n",
        "to store states and actions for each transition. Fill in the missing sample function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uFyATAS4QVW",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=100_000):\n",
        "        self._max_size = max_size\n",
        "        self._states = None\n",
        "        self._actions = None\n",
        "\n",
        "    def add_rollouts(self, rollouts):\n",
        "        \"\"\"\n",
        "        Add rollouts to the buffer\n",
        "\n",
        "        Args:\n",
        "            rollouts: dict, with keys \"states\" and \"actions\", with shapes\n",
        "                (rollout_length, state_dim) and (rollout_length, action_dim)\n",
        "                respectively.\n",
        "        \"\"\"\n",
        "        if self._states is None:\n",
        "            self._states = rollouts[\"states\"][-self._max_size :]\n",
        "            self._actions = rollouts[\"actions\"][-self._max_size :]\n",
        "        else:\n",
        "            self._states = np.concatenate([self._states, rollouts[\"states\"]])[\n",
        "                -self._max_size :\n",
        "            ]\n",
        "            self._actions = np.concatenate([self._actions, rollouts[\"actions\"]])[\n",
        "                -self._max_size :\n",
        "            ]\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Sample batch_size elements from the buffer without replacement.\n",
        "\n",
        "        Args:\n",
        "            batch_size: int, number of elements to sample\n",
        "        Returns:\n",
        "            states: np.array of shape (batch_size, state_dim)\n",
        "            actions: np.array of shape (batch_size, action_dim)\n",
        "        \"\"\"\n",
        "        if self._states is None or self._actions is None:\n",
        "            raise ValueError(\"No data in buffer\")\n",
        "\n",
        "        # TODO: Sample batch_size random elements from self.states and self.actions\n",
        "        indices = np.random.choice(len(self._states), batch_size, replace=False)\n",
        "        states = self._states[indices]\n",
        "        actions = self._actions[indices]\n",
        "        return states, actions\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._states) if self._states is not None else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "2j_y1zmr4QVW"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q1b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "VWTmnR2v4QVW"
      },
      "source": [
        "## Q1.c: Agent (3 pts)\n",
        "Finally, we come to the agent, which is the entity that selects actions to perform in\n",
        "the environment. We've provided the network architecture below. It's up to you to fill\n",
        "in the agent's `forward` and `get_action` functions. They do similar things, but keep in\n",
        "mind the expected function signature!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYhHhbOY4QVW",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, obs_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self._network = torch.nn.Sequential(\n",
        "            torch.nn.Linear(obs_dim, 256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, 256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, action_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, obs_tensor: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Returns the actions for a batch of observations.\n",
        "\n",
        "        Args:\n",
        "            obs_tensor: torch.Tensor of shape (batch_size, obs_dim)\n",
        "        Returns:\n",
        "            action_tensor: torch.Tensor of shape (batch_size, action_dim)\n",
        "        \"\"\"\n",
        "        return self._network(obs_tensor)\n",
        "\n",
        "    def get_action(self, obs: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get action from the agent for a single observation.\n",
        "\n",
        "        Args:\n",
        "            obs: np.ndarray of shape (obs_dim,)\n",
        "        Returns:\n",
        "            action: np.ndarray of shape (action_dim,)\n",
        "        \"\"\"\n",
        "\n",
        "        obs = torch.tensor(obs, dtype=torch.float32)\n",
        "        return self.forward(obs).cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "AyfLCSK44QVW"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q1c\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZKZado34QVW"
      },
      "source": [
        "# Q2: Behavior cloning (20 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ecj_hGxg4QVW"
      },
      "source": [
        "## Q2.a Implement Behavior Cloning (15 pts)\n",
        "We now come to our first Imitation Learning algorithm: behavior cloning.\n",
        "Run `steps` steps of gradient descent using the `optimizer` with the predictions\n",
        "coming from the `agent` and input and targets coming from the `buffer` in batch sizes of\n",
        "`batch_size`. Since this is a continuous action space, we will be using a regression\n",
        "loss, specifically average mean squared error:\n",
        "$l(\\mathbf{x}, \\mathbf{y}) = \\frac{\\sum_{m=1}^M\\sum_{n=1}^N (x_n^m - y_n^m)^2}{N\\times\n",
        "M}$, where $M$ is the batch size, $N$ is the dimension of each sample, and $x_n^m$\n",
        "refers to the $n$-th dimension of the $m$-th sample.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "6BR709764QVW",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def behavior_cloning(agent, optimizer, buffer, batch_size=128, steps=1000):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        agent: Agent, agent to train\n",
        "        optimizer: torch.optim.Optimizer, optimizer to use\n",
        "        buffer: ReplayBuffer, buffer to sample from\n",
        "        batch_size: int, batch size\n",
        "        steps: int, number of steps to train\n",
        "    Returns:\n",
        "        loss: float, Average loss over the last 5 steps\n",
        "    \"\"\"\n",
        "\n",
        "    losses = []\n",
        "    # TODO: Implement the behavior cloning training loop\n",
        "    # Hint: Store the loss values in losses list to compute the final average over the\n",
        "    # last 5 steps\n",
        "    # Hint: Take a look at torch.nn.functional for useful functions for computing the\n",
        "    # loss\n",
        "    for step in tqdm(range(steps), desc=\"Training agent\", position=1, leave=False):\n",
        "        states, actions = buffer.sample(batch_size)\n",
        "        states = torch.tensor(states, dtype=torch.float32)\n",
        "        actions = torch.tensor(actions, dtype=torch.float32)\n",
        "        actions_pred = agent(states)\n",
        "        loss = torch.nn.functional.mse_loss(actions_pred, actions)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "    \n",
        "    return np.mean(losses[-5:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "tdSwAoAi4QVW"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q2.a\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "_c5NkGe44QVW"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "## Q2.b Run Behavior Cloning (5 pts)\n",
        "Run behavior cloning on the curated data given above for 1000 steps. Then evaluate the agent for\n",
        "10 episodes, reporting the mean and standard deviation. You should get at least 50% of\n",
        "the average expert return."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "HNpN_XvH4QVW",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "with open(\"./public/a2/expert_data/expert_data_Ant-v4.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "    states = np.concatenate([trajectory[\"observation\"][:, :27] for trajectory in data])\n",
        "    actions = np.concatenate([trajectory[\"action\"] for trajectory in data])\n",
        "    data_average_reward = np.mean([np.sum(trajectory[\"reward\"]) for trajectory in data])\n",
        "print(f\"Average expert return: {data_average_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvPpsy9z4QVW",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "STEPS = 1000\n",
        "bc_agent = Agent(env.observation_space.shape[0], env.action_space.shape[0])\n",
        "optimizer = torch.optim.Adam(bc_agent.parameters(), lr=5e-3)\n",
        "bc_buffer = ReplayBuffer()\n",
        "\n",
        "bc_buffer.add_rollouts({\"states\": states, \"actions\": actions})\n",
        "loss = behavior_cloning(bc_agent, optimizer, bc_buffer, batch_size=BATCH_SIZE, steps=STEPS)\n",
        "mean, std = evaluate_agent(bc_agent, env, 10)\n",
        "\n",
        "print(\n",
        "    f\"The agent trained on the curated dataset has an average reward of {mean} +/- {std}\"\n",
        ")\n",
        "create_video(vis_env, bc_agent, name_prefix=\"ant_curated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "2eL9Vlux4QVW"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "# Q3 DAgger Implementation (30 pts)\n",
        "Finally, we look at the [Dataset Aggregation (DAgger)\n",
        "algorithm](https://www.ri.cmu.edu/pub_files/2011/4/Ross-AISTATS11-NoRegret.pdf). Each\n",
        "iteration of this algorithm involves dataset collection, data relabeling with an expert\n",
        "policy, and behavior cloning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvLETBZn4QVX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def relabel_with_expert(states, expert_agent):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        states: np.array of shape (batch_size, state_dim)\n",
        "        expert_agent: ExpertAgent\n",
        "    Returns:\n",
        "        actions: np.array of shape (batch_size, action_dim)\n",
        "    \"\"\"\n",
        "    actions = []\n",
        "\n",
        "    # TODO: Loop through the states, and get the expert action\n",
        "    # for each state\n",
        "    # Hint: Use expert_agent.get_action\n",
        "\n",
        "    for state in states:\n",
        "        action = expert_agent.get_action(state)\n",
        "        actions.append(action)\n",
        "\n",
        "    return np.array(actions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "9tmWd0IF4QVX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def collect_rollouts(env, agent, n_to_collect=1000):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        env: gym.Env\n",
        "        agent: Agent\n",
        "        n_to_collect: int, number of states to collect\n",
        "    Returns:\n",
        "        states: np.array of shape (n_to_collect, state_dim)\n",
        "        actions: np.array of shape (n_to_collect, action_dim)\n",
        "    \"\"\"\n",
        "    states = []\n",
        "    actions = []\n",
        "    state, info = env.reset()\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "\n",
        "    ### TODO: Collect rollouts until we have n_to_collect states\n",
        "    # Hint: Remember to reset the environment when a rollout is finished\n",
        "\n",
        "    for _ in tqdm(range(n_to_collect), desc=\"Collecting rollouts\", position=1, leave=False):\n",
        "        action = agent.get_action(state)\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        state = next_state\n",
        "        if terminated or truncated:\n",
        "            state, info = env.reset()\n",
        "\n",
        "    return np.array(states), np.array(actions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZIpp0Cg4QVX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def seed_data(env, expert_agent, buffer, n_to_collect=1000):\n",
        "    \"\"\"\n",
        "    Collects rollouts using the expert agent and adds them to the buffer.\n",
        "\n",
        "    Args:\n",
        "        env: gym.Env\n",
        "        expert_agent: ExpertAgent\n",
        "        buffer: ReplayBuffer\n",
        "        n_to_collect: int, number of samples to collect\n",
        "    \"\"\"\n",
        "\n",
        "    states, actions = collect_rollouts(env, expert_agent, n_to_collect)\n",
        "    buffer.add_rollouts({\"states\": states, \"actions\": actions})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "edDv2rGL4QVX"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ZJH63e8F4QVX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def dagger_iteration(\n",
        "    agent,\n",
        "    optimizer,\n",
        "    expert_agent,\n",
        "    env,\n",
        "    buffer,\n",
        "    n_to_collect,\n",
        "    steps=1000,\n",
        "    batch_size=128,\n",
        "):\n",
        "    \"\"\"\n",
        "    Implements one iteration of the DAgger algorithm. Collects the rollouts using the\n",
        "    agent, relabels them using the expert, and trains the agent for `steps` steps using\n",
        "    behavior cloning.\n",
        "\n",
        "    Args:\n",
        "        agent: Agent\n",
        "        optimizer: torch.optim.Optimizer\n",
        "        expert_agent: ExpertAgent\n",
        "        env: gym.Env\n",
        "        buffer: ReplayBuffer\n",
        "        n_to_collect: int, number of samples to collect\n",
        "        steps: int, number of steps to train\n",
        "        batch_size: int, batch size\n",
        "    Returns:\n",
        "        loss: float, Average loss over the last 5 steps of behavior\n",
        "            cloning\n",
        "    \"\"\"\n",
        "\n",
        "    seed_data(env, expert_agent, buffer, n_to_collect)\n",
        "    loss = behavior_cloning(agent, optimizer, buffer, batch_size, steps)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "0Boft-i54QVX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def dagger(\n",
        "    agent,\n",
        "    optimizer,\n",
        "    expert_agent,\n",
        "    env,\n",
        "    buffer,\n",
        "    collect_per_iteration=2000,\n",
        "    n_iterations=10,\n",
        "    gradient_steps=1000,\n",
        "    batch_size=128,\n",
        "    n_episodes_eval=10,\n",
        "):\n",
        "    \"\"\"\n",
        "    Runs the DAgger algorithm for `n_iterations` iterations. The loss from each\n",
        "    iteration is stored and returned. After each iteration, the agent is evaluated for\n",
        "    `n_episodes_eval` episodes. The mean and std of the rewards are stored and returned.\n",
        "\n",
        "    Args:\n",
        "        agent: Agent\n",
        "        optimizer: torch.optim.Optimizer\n",
        "        expert_agent: ExpertAgent\n",
        "        env: gym.Env\n",
        "        buffer: ReplayBuffer\n",
        "        collect_per_iteration: int, number of samples to collect per iteration\n",
        "        n_iterations: int, number of DAgger iterations\n",
        "        gradient_steps: int, number of steps to train the agent for per iteration\n",
        "        batch_size: int, batch size\n",
        "        n_episodes_eval: int, number of episodes to evaluate the agent for\n",
        "    Returns:\n",
        "        losses: list of floats, losses from each DAgger iteration\n",
        "        means: list of floats, mean rewards from each DAgger iteration\n",
        "        stds: list of floats, std of rewards from each DAgger iteration\n",
        "    \"\"\"\n",
        "    losses, means, stds = [], [], []\n",
        "\n",
        "    ### TODO: Implement the DAgger algorithm\n",
        "    # Hint: It might be helpful when running stuff later on to also print\n",
        "    # which iteration of DAgger you are on\n",
        "    for _ in tqdm(range(n_iterations), desc=\"DAgger iteration\", position=0):\n",
        "        loss = dagger_iteration(\n",
        "            agent,\n",
        "            optimizer,\n",
        "            expert_agent,\n",
        "            env,\n",
        "            buffer,\n",
        "            collect_per_iteration,\n",
        "            gradient_steps,\n",
        "            batch_size,\n",
        "        )\n",
        "        losses.append(loss)\n",
        "        mean, std = evaluate_agent(agent, env, n_episodes_eval)\n",
        "        means.append(mean)\n",
        "        stds.append(std)\n",
        "\n",
        "    return losses, means, stds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ouQ3SbH-4QVX"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "# Q4 Analyzing DAgger\n",
        "Now, you will perform various experiments to test and analyze the performance of\n",
        "behavior cloning and DAgger."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "wq0JzrfD4QVX"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "## Q4.a: DAgger with policy drift\n",
        "You currently have access to two agents: the `expert_1mil` policy that we provided you,\n",
        "and the `bc_agent` learned through behavior cloning the curated expert data. Starting from\n",
        "the same agent and replay buffer as the behavior cloning experiment above, run 15\n",
        "iterations of DAgger with the `expert_1mil` policy. Then, reset the agent and buffer, do\n",
        "15 iterations of DAgger with the `expert_1mil` policy starting from a random agent and\n",
        "empty replay buffer. Plot the loss and average mean with standard deviation using the\n",
        "plotting function above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "3rrpBni44QVX",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DAgger iteration:  60%|██████    | 9/15 [02:09<01:26, 14.41s/it]"
          ]
        }
      ],
      "source": [
        "# Run DAgger starting from agent pretrained on curated data data\n",
        "expert = ExpertAgent(\"./public/a2/experts/network_1mil.pt\")\n",
        "agent = bc_agent\n",
        "buffer = bc_buffer\n",
        "optimizer = torch.optim.Adam(agent.parameters(), lr=5e-3)\n",
        "losses_bc, means_bc, stds_bc = dagger(\n",
        "    agent, optimizer, expert, env, buffer, 2000, 15, 2000, 128, 10\n",
        ")\n",
        "\n",
        "# Run DAgger starting from scratch, using the same expert\n",
        "agent = Agent(env.observation_space.shape[0], env.action_space.shape[0])\n",
        "buffer = ReplayBuffer()\n",
        "optimizer = torch.optim.Adam(agent.parameters(), lr=5e-3)\n",
        "seed_data(env, expert, buffer, 2000)\n",
        "losses_scratch, means_scratch, stds_scratch = dagger(\n",
        "    agent, optimizer, expert, env, buffer, 2000, 15, 2000, 128, 10\n",
        ")\n",
        "plot(\n",
        "    [np.arange(len(losses_bc)), np.arange(len(losses_scratch))],\n",
        "    [means_bc, means_scratch],\n",
        "    [stds_bc, stds_scratch],\n",
        "    [losses_bc, losses_scratch],\n",
        "    [\"BC\", \"Scratch\"],\n",
        "    running_average=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPHPcnnS4QVX"
      },
      "source": [
        "_Type your answer here, replacing this text._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "DOncGBnF4QVX"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "For the rest of this assignment, we will be using a new expert agent. Evaluate and visualize it below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oh8Nzxu94QVX",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "expert_2mil = ExpertAgent(\"./public/a2/experts/network_2mil.pt\")\n",
        "mean, std = evaluate_agent(expert_2mil, env, 10)\n",
        "print(f\"Expert mean return: {mean} +/- {std}\")\n",
        "create_video(vis_env, expert_2mil, \"expert_2mil\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "SvXZhuQQ4QVX"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "# Q4.b Exploring the effect of the effect of the strength of the expert on DAgger\n",
        "We now look at how the strength of the expert affects our imitation learned algorithm.\n",
        "The `expert_1mil` and `expert_2mil` are both policies from the same training run, except\n",
        "the `expert_1mil` was trained for 1 million steps and `expert_2mil` was trained for 2 million steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEJ0FkK04QVX"
      },
      "source": [
        "_Type your answer here, replacing this text._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkodcjz94QVX",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "N_ITERS = 50\n",
        "N_DATA_PER_ITER = 2000\n",
        "N_GRADIENT_STEPS = 2000\n",
        "expert_strength_data = {\n",
        "    \"all_means\": [],\n",
        "    \"all_stds\": [],\n",
        "    \"all_losses\": [],\n",
        "    \"all_xs\": [],\n",
        "}\n",
        "for expert in [expert_1mil, expert_2mil]:\n",
        "    agent = Agent(env.observation_space.shape[0], env.action_space.shape[0])\n",
        "    optimizer = torch.optim.Adam(agent.parameters(), lr=5e-3)\n",
        "    buffer = ReplayBuffer()\n",
        "    seed_data(env, expert, buffer, 2000)\n",
        "\n",
        "    # TODO: Run DAgger for the given expert\n",
        "    losses, means, stds = dagger(\n",
        "        agent, optimizer, expert, env, buffer, 2000, 50, 2000, 128, 10\n",
        "    )\n",
        "\n",
        "    xs = np.arange(N_ITERS) + 1\n",
        "    expert_strength_data[\"all_xs\"].append(xs)\n",
        "    expert_strength_data[\"all_means\"].append(means)\n",
        "    expert_strength_data[\"all_stds\"].append(stds)\n",
        "    expert_strength_data[\"all_losses\"].append(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sceVO5ys4QVX",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "plot(\n",
        "    expert_strength_data[\"all_xs\"],\n",
        "    expert_strength_data[\"all_means\"],\n",
        "    expert_strength_data[\"all_stds\"],\n",
        "    expert_strength_data[\"all_losses\"],\n",
        "    [f\"{expert} expert\" for expert in [\"1mil\", \"2mil\"]],\n",
        "    min=-1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "mI67mAiB4QVX"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "## Q4.c Exploring the effect of the number of iterations on DAgger\n",
        "We will now look at how the frequency of the number of DAgger iterations affects the\n",
        "performance. To make it fair, make sure to control for the total amount of data and\n",
        "gradient steps that will be taken by the algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqFTK-q44QVY"
      },
      "source": [
        "_Type your answer here, replacing this text._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEAB79vh4QVY",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "TOTAL_DATA = 100_000\n",
        "TOTAL_GRADIENT_STEPS = 100_000\n",
        "n_iters_data = {\n",
        "    \"all_means\": [],\n",
        "    \"all_stds\": [],\n",
        "    \"all_losses\": [],\n",
        "    \"all_xs\": [],\n",
        "}\n",
        "expert = ExpertAgent(\"./public/a2/experts/network_2mil.pt\")\n",
        "for n_iters in [5, 25, 50, 100, 200]:\n",
        "    agent = Agent(env.observation_space.shape[0], env.action_space.shape[0])\n",
        "    optimizer = torch.optim.Adam(agent.parameters(), lr=5e-3)\n",
        "    buffer = ReplayBuffer()\n",
        "    seed_data(env, expert, buffer, 2000)\n",
        "\n",
        "    # TODO: Run DAgger for n_iters iterations\n",
        "    losses, means, stds = dagger(\n",
        "        agent, optimizer, expert, env, buffer, 2000, n_iters, 2000, 128, 10\n",
        "    )\n",
        "    xs = 100_000 / n_iters * (np.arange(n_iters) + 1)\n",
        "    n_iters_data[\"all_xs\"].append(xs)\n",
        "    n_iters_data[\"all_means\"].append(means)\n",
        "    n_iters_data[\"all_stds\"].append(stds)\n",
        "    n_iters_data[\"all_losses\"].append(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gI9JXtUa4QVY",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "plot(\n",
        "    n_iters_data[\"all_xs\"],\n",
        "    n_iters_data[\"all_means\"],\n",
        "    n_iters_data[\"all_stds\"],\n",
        "    n_iters_data[\"all_losses\"],\n",
        "    [f\"{n_iters} iters\" for n_iters in [5, 25, 50, 100, 200]],\n",
        "    min=-1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "29o3ie2N4QVY"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
