{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/Depdx/INF8250AE--Reinforcement-Learning/blob/main/assignement-2/assignement_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx--OudV4QVT"
      },
      "source": [
        "# Assignment 2: Imitation Learning\n",
        "In this assignment, you will implement the basic components of an Imitation Learning\n",
        "system, behavior cloning, and DAgger.\n",
        "## Instructions\n",
        "- This is an individual assignment. You are not allowed to discuss the problems with other students.\n",
        "- Part of this assignment will be autograded by gradescope. You can use it as immediate feedback to improve your answers. You can resubmit as many times as you want.\n",
        "- All your solution, code, analysis, graphs, explanations should be done in this same notebook.\n",
        "- Please make sure to execute all the cells before you submit the notebook to the gradescope. - You will not get points for the plots if they are not generated already.\n",
        "- If you have questions regarding the assignment, you can ask for clarifications in\n",
        "  Piazza. You should use the corresponding tag for this assignment.\n",
        "- Start Early! Some of the cells can take about an hour to run on CPU. You will need\n",
        "  time to generate the results.\n",
        "  \n",
        "**When Submitting to GradeScope**: Be sure to\n",
        "1. Submit a .ipynb notebook to the Assignment 2 - Code section on Gradescope.\n",
        "2. Submit a pdf version of the notebook to the Assignment 2 - Report entry.\n",
        "\n",
        "Note: You can choose to submit responses in either English or French.\n",
        "\n",
        "Before starting the assignment, make sure that you have downloaded all the tests related\n",
        "for the assignment and put them in the appropriate locations. If you run the next cell,\n",
        "we will set this all up automatically for you in a dataset called public, which will\n",
        "contain both the data and tests you use.\n",
        "\n",
        "This assignment has 4 questions. You will learn to:\n",
        "1. Implement basic components in an Imitation Learning/RL setup.\n",
        "2. Implement behavior cloning.\n",
        "3. Implement DAgger.\n",
        "4. Analyze different aspects of the DAgger algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "49FfhdId4QVU",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]      \u001b[0m\u001b[33m\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease                         \u001b[0m\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]        \u001b[0m\n",
            "Hit:5 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InReleasem\u001b[33m\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B][33m\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]      \u001b[0m\u001b[33m\u001b[33m\u001b[33m\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,266 kB]m\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\u001b[33m\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,342 kB][33m\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease   \u001b[0m\u001b[33m\u001b[33m\n",
            "Fetched 2,950 kB in 6s (471 kB/s)                                              \u001b[0m\u001b[33m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "45 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'zlib1g-dev' instead of 'libz-dev'\n",
            "Note, selecting 'ack' instead of 'ack-grep'\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "libxcursor-dev is already the newest version (1:1.2.0-2build4).\n",
            "libxi6 is already the newest version (2:1.8-1build1).\n",
            "libxinerama-dev is already the newest version (2:1.1.4-3).\n",
            "libxrandr2 is already the newest version (2:1.5.2-1build1).\n",
            "lsb-release is already the newest version (11.1.0ubuntu4).\n",
            "make is already the newest version (4.3-4.1build1).\n",
            "wget is already the newest version (1.21.2-2ubuntu1).\n",
            "ack is already the newest version (3.5.0-1).\n",
            "libglew-dev is already the newest version (2.2.0-4).\n",
            "libglfw3 is already the newest version (3.3.6-1).\n",
            "libglfw3-dev is already the newest version (3.3.6-1).\n",
            "patchelf is already the newest version (0.14.3-1).\n",
            "swig is already the newest version (4.0.2-1ubuntu1).\n",
            "xpra is already the newest version (3.1-1build5).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.1).\n",
            "curl is already the newest version (7.81.0-1ubuntu1.13).\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.10).\n",
            "libgl1-mesa-dev is already the newest version (23.0.4-0ubuntu1~22.04.1).\n",
            "libosmesa6-dev is already the newest version (23.0.4-0ubuntu1~22.04.1).\n",
            "unzip is already the newest version (6.0-26ubuntu3.1).\n",
            "xserver-xorg-dev is already the newest version (2:21.1.4-2ubuntu1.7~22.04.1).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-2ubuntu9.2).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "gnupg2 is already the newest version (2.2.27-3ubuntu2.1).\n",
            "libgl1-mesa-glx is already the newest version (23.0.4-0ubuntu1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package python-opengl\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt update\n",
        "!apt install -y --no-install-recommends \\\n",
        "        build-essential \\\n",
        "        curl \\\n",
        "        git \\\n",
        "        gnupg2 \\\n",
        "        make \\\n",
        "        cmake \\\n",
        "        ffmpeg \\\n",
        "        swig \\\n",
        "        libz-dev \\\n",
        "        unzip \\\n",
        "        zlib1g-dev \\\n",
        "        libglfw3 \\\n",
        "        libglfw3-dev \\\n",
        "        libxrandr2 \\\n",
        "        libxinerama-dev \\\n",
        "        libxi6 \\\n",
        "        libxcursor-dev \\\n",
        "        libgl1-mesa-dev \\\n",
        "        libgl1-mesa-glx \\\n",
        "        libglew-dev \\\n",
        "        libosmesa6-dev \\\n",
        "        lsb-release \\\n",
        "        ack-grep \\\n",
        "        patchelf \\\n",
        "        wget \\\n",
        "        xpra \\\n",
        "        xserver-xorg-dev \\\n",
        "        ffmpeg\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jGkhWwWt4QVV",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Requirement already satisfied: mujoco>=2.3.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (2.3.7)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (2.31.3)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (9.4.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (2.6.2)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[mujoco]) (3.1.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.10/dist-packages (3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium[mujoco]\n",
        "!pip install torch\n",
        "!pip install tqdm\n",
        "!pip install matplotlib\n",
        "!pip install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tC9PXCeD4QVV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: otter-grader in /usr/local/lib/python3.10/dist-packages (5.2.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from otter-grader) (0.3.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from otter-grader) (3.1.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from otter-grader) (5.9.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from otter-grader) (1.5.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from otter-grader) (6.0.1)\n",
            "Requirement already satisfied: python-on-whales in /usr/local/lib/python3.10/dist-packages (from otter-grader) (0.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from otter-grader) (2.31.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from otter-grader) (1.15.0)\n",
            "Requirement already satisfied: jupytext in /usr/local/lib/python3.10/dist-packages (from otter-grader) (1.15.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from otter-grader) (8.1.7)\n",
            "Requirement already satisfied: fica>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from otter-grader) (0.3.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from otter-grader) (7.34.0)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.10/dist-packages (from otter-grader) (1.6.3)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from otter-grader) (7.7.1)\n",
            "Requirement already satisfied: ipylab in /usr/local/lib/python3.10/dist-packages (from otter-grader) (1.0.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from otter-grader) (6.5.4)\n",
            "Requirement already satisfied: docutils in /usr/local/lib/python3.10/dist-packages (from fica>=0.3.0->otter-grader) (0.18.1)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.10/dist-packages (from fica>=0.3.0->otter-grader) (5.0.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse->otter-grader) (0.41.2)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse->otter-grader) (1.16.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->otter-grader) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->otter-grader) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->otter-grader) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->otter-grader) (3.6.5)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->otter-grader) (3.0.8)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (4.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->otter-grader) (2.1.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from jupytext->otter-grader) (0.10.2)\n",
            "Requirement already satisfied: markdown-it-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from jupytext->otter-grader) (3.0.0)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.10/dist-packages (from jupytext->otter-grader) (0.4.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (0.4)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (5.3.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (0.2.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (0.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (23.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->otter-grader) (2.18.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->otter-grader) (4.19.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->otter-grader) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->otter-grader) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->otter-grader) (1.23.5)\n",
            "Requirement already satisfied: pydantic!=2.0.*,<3,>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-on-whales->otter-grader) (1.10.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from python-on-whales->otter-grader) (4.66.1)\n",
            "Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from python-on-whales->otter-grader) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-on-whales->otter-grader) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->otter-grader) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->otter-grader) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->otter-grader) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->otter-grader) (2023.7.22)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->otter-grader) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->otter-grader) (6.3.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->otter-grader) (0.8.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->otter-grader) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->otter-grader) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->otter-grader) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->otter-grader) (0.10.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->otter-grader) (3.10.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=1.0.0->jupytext->otter-grader) (0.1.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->otter-grader) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->otter-grader) (0.2.6)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (6.5.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->otter-grader) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->otter-grader) (0.5.1)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (1.0.7)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (1.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (2.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (1.1.9)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (1.0.6)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (2.12.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (0.7.13)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (1.4.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets->otter-grader) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (23.1.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.5.7)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (0.17.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.0.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (0.2.3)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (21.2.0)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.6.2)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.15.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.1.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (2.21)\n",
            "fatal: destination path 'public' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!pip install otter-grader\n",
        "!git clone https://github.com/chandar-lab/INF8250ae-assignments-2023.git public"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "roxsb0-C4QVV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f505cf1bf70>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title set up virtual display\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BabfQXQv4QVV"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import wrappers\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import os\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "a6_cQz0p4QVV",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import otter\n",
        "grader = otter.Notebook(colab=True, tests_dir='./public/a2/tests')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Wf-jc3rk4QVV"
      },
      "outputs": [],
      "source": [
        "def plot(\n",
        "    xs_list,\n",
        "    means_list,\n",
        "    stds_list,\n",
        "    losses_list,\n",
        "    labels_list=None,\n",
        "    min=None,\n",
        "    running_average=5,\n",
        "):\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    if labels_list is None:\n",
        "        labels_list = [f\"Agent {idx}\" for idx in range(len(means_list))]\n",
        "    for xs, means, stds, losses, label in zip(\n",
        "        xs_list, means_list, stds_list, losses_list, labels_list\n",
        "    ):\n",
        "        kernel = np.ones(running_average) / running_average\n",
        "        means_convolved = np.convolve(means, kernel, mode=\"same\")\n",
        "        stds_convolved = np.convolve(stds, kernel, mode=\"same\")\n",
        "        ax[0].plot(xs, means_convolved, label=label)\n",
        "        ax[0].fill_between(\n",
        "            xs,\n",
        "            np.array(means_convolved) - np.array(stds_convolved),\n",
        "            np.array(means_convolved) + np.array(stds_convolved),\n",
        "            alpha=0.5,\n",
        "        )\n",
        "        ax[1].plot(xs, losses, label=label)\n",
        "    if min is not None:\n",
        "        ax[0].set_ylim(min, None)\n",
        "    ax[0].legend()\n",
        "    ax[0].set_ylabel(\"Reward\")\n",
        "    ax[1].set_ylabel(\"Loss\")\n",
        "\n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vml_nyOL4QVV"
      },
      "outputs": [],
      "source": [
        "class ExpertAgent(torch.nn.Module):\n",
        "    def __init__(self, filename):\n",
        "        super().__init__()\n",
        "        self._network = torch.load(filename)\n",
        "        self._network.eval()\n",
        "\n",
        "    def get_action(self, obs: np.array):\n",
        "        \"\"\"\n",
        "        Get action from the expert agent.\n",
        "\n",
        "        Args:\n",
        "            obs: np.array of shape (state_dim,)\n",
        "        Returns:\n",
        "            action: np.array of shape (action_dim,)\n",
        "        \"\"\"\n",
        "        obs = torch.tensor(obs, dtype=torch.float32)\n",
        "        return self._network(obs).cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNCrH_sC4QVV"
      },
      "source": [
        "# Q1 Getting started with RL (10 pts)\n",
        "For this assignment, we will be using the [Ant-v4](https://gymnasium.farama.org/environments/mujoco/ant/) environment. The goal in this environment is to have the \"Ant\" run as far as it can for 1000\n",
        "timesteps, with the reward being a linear combination of how far it ran, how long it was\n",
        "in a \"healthy\" state, and a penalty for taking actions  that are too large. The actions\n",
        "control the torque for the motors at each of the 8 joints of the agent.\n",
        "\n",
        "This environment is part of the [gymnasium](https://gymnasium.farama.org/) package, a\n",
        "library which provides a standard interface for environments used across many different\n",
        "RL research projects. For this assignment, you will need to familiar with the interface\n",
        "provided by the [Env](https://gymnasium.farama.org/api/env/) class. Specifically,\n",
        "`env.reset()` and `env.step()`. `env.reset()` resets the environment and agent to the\n",
        "start of the episode. It does not have any required arguments, and it returns `(obs, info)`,\n",
        "where `obs` is the first observation of the episode, and `info` is a dictionary\n",
        "containing additional information (you will not need to interact with `info`). To\n",
        "take actions in the environment, call `env.step`, which takes in an action, and returns\n",
        "`(obs, reward, terminated, truncated, info)`, where `obs` is the next state, `reward` is\n",
        "the reward for step just taken, `terminated` refers to whether the episode entered a\n",
        "terminal state, `truncated` refers to whether the episode was ended before entering a\n",
        "terminal state, and `info` contains any extra info the environment wants to provide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "siHzBV8b4QVV"
      },
      "source": [
        "## Q1.a: Agent Evaluation (4 pts)\n",
        "As a warmup and introduction to interactive environments, implement the `evaluate_agent`\n",
        "function below. It should collect `num_episodes` trajectories in the environment, and\n",
        "return the mean and standard deviation of the episode returns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "m_jYoBj-4QVW",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def evaluate_agent(agent, env:gym.Env, num_episodes:int):\n",
        "    \"\"\" Collect num_episodes trajectories for the agent and compute mean and std of the\n",
        "    rewards. Remember to reset the environment before each episode.\n",
        "    Args:\n",
        "        agent: Agent, agent to evaluate\n",
        "        env: gym.Env, environment to evaluate agent on\n",
        "        num_episodes: int, number of episodes to evaluate the agent for\n",
        "    Returns:\n",
        "        mean_return: float, mean return over the episodes\n",
        "        std_return: float, standard deviation of the return over the episodes\n",
        "    \"\"\"\n",
        "    returns = []\n",
        "    for _ in range(num_episodes):\n",
        "        obs, info = env.reset()\n",
        "        terminated = False\n",
        "        total_reward = 0\n",
        "        while not terminated or not truncated:\n",
        "            action = agent.get_action(obs)\n",
        "            obs, reward, terminated, truncated, info = env.step(action)\n",
        "            total_reward += reward\n",
        "        returns.append(total_reward)\n",
        "    mean_return = np.mean(returns)\n",
        "    std_return = np.std(returns)\n",
        "    return mean_return, std_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "t5fZSuoD4QVW"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-3140aae5e100>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"q1a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/otter/check/utils.py\u001b[0m in \u001b[0;36mevent_logger\u001b[0;34m(wrapped, self, args, kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \"\"\"\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mret\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLoggedEventReturnValue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/otter/check/notebook.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(self, question, global_env)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# run the check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Calling checker\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChecker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nbmeta_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mLoggedEventReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshelve_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/otter/execute/checker.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(cls, nb_or_test_path, nbmeta_config, test_name, global_env)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mglobal_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_globals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_track_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/otter/test_files/exception_test.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, global_environment)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mpassed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"✅ Test case passed\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                 \u001b[0mtest_case\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_environment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mpassed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"❌ Test case failed\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_error_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/otter/test_files/exception_test.py\u001b[0m in \u001b[0;36mcall_func\u001b[0;34m(self, global_environment)\u001b[0m\n\u001b[1;32m    112\u001b[0m         call_kwargs = {arg: (global_environment if arg == \"env\" else \\\n\u001b[1;32m    113\u001b[0m                 global_environment.get(arg, None)) for arg in args}\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/public/a2/tests/q1a.py\u001b[0m in \u001b[0;36mtest_evaluate_agent_public\u001b[0;34m(gym, np, ExpertAgent, evaluate_agent)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mexpert_1mil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExpertAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./public/a2/experts/network_1mil.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpert_1mil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The mean return of the expert should be greater than 4000\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The standard deviation of the expert should not be 0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-a3d5f1429c8c>\u001b[0m in \u001b[0;36mevaluate_agent\u001b[0;34m(agent, env, num_episodes)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-367f99da4e90>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maction_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \"\"\"\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "grader.check(\"q1a\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCjDny-C4QVW"
      },
      "outputs": [],
      "source": [
        "VIDEO_LOCATION = \"./content/video\"\n",
        "\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob(f\"{VIDEO_LOCATION}/*.mp4\")\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, \"r+b\").read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(\n",
        "            HTML(\n",
        "                data=\"\"\"<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>\"\"\".format(\n",
        "                    encoded.decode(\"ascii\")\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def create_video(vis_env, agent, name_prefix=\"imitation_learning\"):\n",
        "    vis_env = wrappers.RecordVideo(vis_env, VIDEO_LOCATION, name_prefix=name_prefix)\n",
        "    evaluate_agent(agent, vis_env, 1)\n",
        "    vis_env.close_video_recorder()\n",
        "    show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq-8LLn74QVW"
      },
      "source": [
        "Let's now visualize what this looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-d1JNjJ4QVW",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Ant-v4\")\n",
        "vis_env = gym.make(\"Ant-v4\", render_mode=\"rgb_array\")\n",
        "a = env.action_space\n",
        "expert_1mil = ExpertAgent(\"./public/a2/experts/network_1mil.pt\")\n",
        "mean, std = evaluate_agent(expert_1mil, env, 10)\n",
        "print(f\"Expert mean return: {mean} +/- {std}\")\n",
        "create_video(vis_env, expert_1mil, \"expert_1mil\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "5IT_ojCr4QVW"
      },
      "source": [
        "## Q1.b: Replay Buffer (3 pts)\n",
        "Next, we will implement a replay buffer. In RL, we typically store states, actions,\n",
        "rewards, next states, and termination for each transition, but for this assignment,\n",
        "because we are only doing imitation learning (not learning from rewards!), we only need\n",
        "to store states and actions for each transition. Fill in the missing sample function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uFyATAS4QVW",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=100_000):\n",
        "        self._max_size = max_size\n",
        "        self._states = None\n",
        "        self._actions = None\n",
        "\n",
        "    def add_rollouts(self, rollouts):\n",
        "        \"\"\"\n",
        "        Add rollouts to the buffer\n",
        "\n",
        "        Args:\n",
        "            rollouts: dict, with keys \"states\" and \"actions\", with shapes\n",
        "                (rollout_length, state_dim) and (rollout_length, action_dim)\n",
        "                respectively.\n",
        "        \"\"\"\n",
        "        if self._states is None:\n",
        "            self._states = rollouts[\"states\"][-self._max_size :]\n",
        "            self._actions = rollouts[\"actions\"][-self._max_size :]\n",
        "        else:\n",
        "            self._states = np.concatenate([self._states, rollouts[\"states\"]])[\n",
        "                -self._max_size :\n",
        "            ]\n",
        "            self._actions = np.concatenate([self._actions, rollouts[\"actions\"]])[\n",
        "                -self._max_size :\n",
        "            ]\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Sample batch_size elements from the buffer without replacement.\n",
        "\n",
        "        Args:\n",
        "            batch_size: int, number of elements to sample\n",
        "        Returns:\n",
        "            states: np.array of shape (batch_size, state_dim)\n",
        "            actions: np.array of shape (batch_size, action_dim)\n",
        "        \"\"\"\n",
        "        if self._states is None or self._actions is None:\n",
        "            raise ValueError(\"No data in buffer\")\n",
        "\n",
        "        # TODO: Sample batch_size random elements from self.states and self.actions\n",
        "\n",
        "        return states, actions\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._states) if self._states is not None else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "2j_y1zmr4QVW"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q1b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "VWTmnR2v4QVW"
      },
      "source": [
        "## Q1.c: Agent (3 pts)\n",
        "Finally, we come to the agent, which is the entity that selects actions to perform in\n",
        "the environment. We've provided the network architecture below. It's up to you to fill\n",
        "in the agent's `forward` and `get_action` functions. They do similar things, but keep in\n",
        "mind the expected function signature!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYhHhbOY4QVW",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Agent(torch.nn.Module):\n",
        "    def __init__(self, obs_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self._network = torch.nn.Sequential(\n",
        "            torch.nn.Linear(obs_dim, 256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, 256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, action_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, obs_tensor: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Returns the actions for a batch of observations.\n",
        "\n",
        "        Args:\n",
        "            obs_tensor: torch.Tensor of shape (batch_size, obs_dim)\n",
        "        Returns:\n",
        "            action_tensor: torch.Tensor of shape (batch_size, action_dim)\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        pass\n",
        "\n",
        "    def get_action(self, obs: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get action from the agent for a single observation.\n",
        "\n",
        "        Args:\n",
        "            obs: np.ndarray of shape (obs_dim,)\n",
        "        Returns:\n",
        "            action: np.ndarray of shape (action_dim,)\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO Predict the action given the observation\n",
        "        # pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "AyfLCSK44QVW"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q1c\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZKZado34QVW"
      },
      "source": [
        "# Q2: Behavior cloning (20 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ecj_hGxg4QVW"
      },
      "source": [
        "## Q2.a Implement Behavior Cloning (15 pts)\n",
        "We now come to our first Imitation Learning algorithm: behavior cloning.\n",
        "Run `steps` steps of gradient descent using the `optimizer` with the predictions\n",
        "coming from the `agent` and input and targets coming from the `buffer` in batch sizes of\n",
        "`batch_size`. Since this is a continuous action space, we will be using a regression\n",
        "loss, specifically average mean squared error:\n",
        "$l(\\mathbf{x}, \\mathbf{y}) = \\frac{\\sum_{m=1}^M\\sum_{n=1}^N (x_n^m - y_n^m)^2}{N\\times\n",
        "M}$, where $M$ is the batch size, $N$ is the dimension of each sample, and $x_n^m$\n",
        "refers to the $n$-th dimension of the $m$-th sample.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BR709764QVW",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def behavior_cloning(agent, optimizer, buffer, batch_size=128, steps=1000):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        agent: Agent, agent to train\n",
        "        optimizer: torch.optim.Optimizer, optimizer to use\n",
        "        buffer: ReplayBuffer, buffer to sample from\n",
        "        batch_size: int, batch size\n",
        "        steps: int, number of steps to train\n",
        "    Returns:\n",
        "        loss: float, Average loss over the last 5 steps\n",
        "    \"\"\"\n",
        "\n",
        "    losses = []\n",
        "    # TODO: Implement the behavior cloning training loop\n",
        "    # Hint: Store the loss values in losses list to compute the final average over the\n",
        "    # last 5 steps\n",
        "    # Hint: Take a look at torch.nn.functional for useful functions for computing the\n",
        "    # loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "tdSwAoAi4QVW"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q2.a\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "_c5NkGe44QVW"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "## Q2.b Run Behavior Cloning (5 pts)\n",
        "Run behavior cloning on the curated data given above for 1000 steps. Then evaluate the agent for\n",
        "10 episodes, reporting the mean and standard deviation. You should get at least 50% of\n",
        "the average expert return."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "HNpN_XvH4QVW",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "with open(\"./public/a2/expert_data/expert_data_Ant-v4.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "    states = np.concatenate([trajectory[\"observation\"][:, :27] for trajectory in data])\n",
        "    actions = np.concatenate([trajectory[\"action\"] for trajectory in data])\n",
        "    data_average_reward = np.mean([np.sum(trajectory[\"reward\"]) for trajectory in data])\n",
        "print(f\"Average expert return: {data_average_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvPpsy9z4QVW",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "STEPS = 1000\n",
        "bc_agent = Agent(env.observation_space.shape[0], env.action_space.shape[0])\n",
        "optimizer = torch.optim.Adam(bc_agent.parameters(), lr=5e-3)\n",
        "bc_buffer = ReplayBuffer()\n",
        "\n",
        "# TODO: Add the states and actions from the expert curated data to the buffer.\n",
        "# Then run behavior cloning and evaluate the agent.\n",
        "mean, std = ...\n",
        "\n",
        "print(\n",
        "    f\"The agent trained on the curated dataset has an average reward of {mean} +/- {std}\"\n",
        ")\n",
        "create_video(vis_env, bc_agent, name_prefix=\"ant_curated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "2eL9Vlux4QVW"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "# Q3 DAgger Implementation (30 pts)\n",
        "Finally, we look at the [Dataset Aggregation (DAgger)\n",
        "algorithm](https://www.ri.cmu.edu/pub_files/2011/4/Ross-AISTATS11-NoRegret.pdf). Each\n",
        "iteration of this algorithm involves dataset collection, data relabeling with an expert\n",
        "policy, and behavior cloning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvLETBZn4QVX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def relabel_with_expert(states, expert_agent):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        states: np.array of shape (batch_size, state_dim)\n",
        "        expert_agent: ExpertAgent\n",
        "    Returns:\n",
        "        actions: np.array of shape (batch_size, action_dim)\n",
        "    \"\"\"\n",
        "    actions = []\n",
        "\n",
        "    # TODO: Loop through the states, and get the expert action\n",
        "    # for each state\n",
        "    # Hint: Use expert_agent.get_action\n",
        "\n",
        "    return np.array(actions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tmWd0IF4QVX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def collect_rollouts(env, agent, n_to_collect=1000):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        env: gym.Env\n",
        "        agent: Agent\n",
        "        n_to_collect: int, number of states to collect\n",
        "    Returns:\n",
        "        states: np.array of shape (n_to_collect, state_dim)\n",
        "        actions: np.array of shape (n_to_collect, action_dim)\n",
        "    \"\"\"\n",
        "    states = []\n",
        "    actions = []\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    ### TODO: Collect rollouts until we have n_to_collect states\n",
        "    # Hint: Remember to reset the environment when a rollout is finished\n",
        "\n",
        "    return np.array(states), np.array(actions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZIpp0Cg4QVX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def seed_data(env, expert_agent, buffer, n_to_collect=1000):\n",
        "    \"\"\"\n",
        "    Collects rollouts using the expert agent and adds them to the buffer.\n",
        "\n",
        "    Args:\n",
        "        env: gym.Env\n",
        "        expert_agent: ExpertAgent\n",
        "        buffer: ReplayBuffer\n",
        "        n_to_collect: int, number of samples to collect\n",
        "    \"\"\"\n",
        "\n",
        "    ### TODO: Implement this function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "edDv2rGL4QVX"
      },
      "outputs": [],
      "source": [
        "grader.check(\"q3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "qF5e2Xb-4QVX"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJH63e8F4QVX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def dagger_iteration(\n",
        "    agent,\n",
        "    optimizer,\n",
        "    expert_agent,\n",
        "    env,\n",
        "    buffer,\n",
        "    n_to_collect,\n",
        "    steps=1000,\n",
        "    batch_size=128,\n",
        "):\n",
        "    \"\"\"\n",
        "    Implements one iteration of the DAgger algorithm. Collects the rollouts using the\n",
        "    agent, relabels them using the expert, and trains the agent for `steps` steps using\n",
        "    behavior cloning.\n",
        "\n",
        "    Args:\n",
        "        agent: Agent\n",
        "        optimizer: torch.optim.Optimizer\n",
        "        expert_agent: ExpertAgent\n",
        "        env: gym.Env\n",
        "        buffer: ReplayBuffer\n",
        "        n_to_collect: int, number of samples to collect\n",
        "        steps: int, number of steps to train\n",
        "        batch_size: int, batch size\n",
        "    Returns:\n",
        "        loss: float, Average loss over the last 5 steps of behavior\n",
        "            cloning\n",
        "    \"\"\"\n",
        "\n",
        "    ### TODO: Implement one iteration of the DAgger algorithm\n",
        "    ...\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Boft-i54QVX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def dagger(\n",
        "    agent,\n",
        "    optimizer,\n",
        "    expert_agent,\n",
        "    env,\n",
        "    buffer,\n",
        "    collect_per_iteration=2000,\n",
        "    n_iterations=10,\n",
        "    gradient_steps=1000,\n",
        "    batch_size=128,\n",
        "    n_episodes_eval=10,\n",
        "):\n",
        "    \"\"\"\n",
        "    Runs the DAgger algorithm for `n_iterations` iterations. The loss from each\n",
        "    iteration is stored and returned. After each iteration, the agent is evaluated for\n",
        "    `n_episodes_eval` episodes. The mean and std of the rewards are stored and returned.\n",
        "\n",
        "    Args:\n",
        "        agent: Agent\n",
        "        optimizer: torch.optim.Optimizer\n",
        "        expert_agent: ExpertAgent\n",
        "        env: gym.Env\n",
        "        buffer: ReplayBuffer\n",
        "        collect_per_iteration: int, number of samples to collect per iteration\n",
        "        n_iterations: int, number of DAgger iterations\n",
        "        gradient_steps: int, number of steps to train the agent for per iteration\n",
        "        batch_size: int, batch size\n",
        "        n_episodes_eval: int, number of episodes to evaluate the agent for\n",
        "    Returns:\n",
        "        losses: list of floats, losses from each DAgger iteration\n",
        "        means: list of floats, mean rewards from each DAgger iteration\n",
        "        stds: list of floats, std of rewards from each DAgger iteration\n",
        "    \"\"\"\n",
        "    losses, means, stds = [], [], []\n",
        "\n",
        "    ### TODO: Implement the DAgger algorithm\n",
        "    # Hint: It might be helpful when running stuff later on to also print\n",
        "    # which iteration of DAgger you are on\n",
        "\n",
        "    return losses, means, stds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ouQ3SbH-4QVX"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "# Q4 Analyzing DAgger\n",
        "Now, you will perform various experiments to test and analyze the performance of\n",
        "behavior cloning and DAgger."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "wq0JzrfD4QVX"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "## Q4.a: DAgger with policy drift\n",
        "You currently have access to two agents: the `expert_1mil` policy that we provided you,\n",
        "and the `bc_agent` learned through behavior cloning the curated expert data. Starting from\n",
        "the same agent and replay buffer as the behavior cloning experiment above, run 15\n",
        "iterations of DAgger with the `expert_1mil` policy. Then, reset the agent and buffer, do\n",
        "15 iterations of DAgger with the `expert_1mil` policy starting from a random agent and\n",
        "empty replay buffer. Plot the loss and average mean with standard deviation using the\n",
        "plotting function above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "3rrpBni44QVX",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "# Run DAgger starting from agent pretrained on curated data data\n",
        "expert = ExpertAgent(\"./public/a2/experts/network_1mil.pt\")\n",
        "agent = bc_agent\n",
        "buffer = bc_buffer\n",
        "optimizer = torch.optim.Adam(agent.parameters(), lr=5e-3)\n",
        "losses_bc, means_bc, stds_bc = dagger(\n",
        "    agent, optimizer, expert, env, buffer, 2000, 15, 2000, 128, 10\n",
        ")\n",
        "\n",
        "# Run DAgger starting from scratch, using the same expert\n",
        "agent = Agent(env.observation_space.shape[0], env.action_space.shape[0])\n",
        "buffer = ReplayBuffer()\n",
        "optimizer = torch.optim.Adam(agent.parameters(), lr=5e-3)\n",
        "seed_data(env, expert, buffer, 2000)\n",
        "losses_scratch, means_scratch, stds_scratch = dagger(\n",
        "    agent, optimizer, expert, env, buffer, 2000, 15, 2000, 128, 10\n",
        ")\n",
        "plot(\n",
        "    [np.arange(len(losses_bc)), np.arange(len(losses_scratch))],\n",
        "    [means_bc, means_scratch],\n",
        "    [stds_bc, stds_scratch],\n",
        "    [losses_bc, losses_scratch],\n",
        "    [\"BC\", \"Scratch\"],\n",
        "    running_average=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPHPcnnS4QVX"
      },
      "source": [
        "_Type your answer here, replacing this text._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "DOncGBnF4QVX"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "For the rest of this assignment, we will be using a new expert agent. Evaluate and visualize it below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oh8Nzxu94QVX",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "expert_2mil = ExpertAgent(\"./public/a2/experts/network_2mil.pt\")\n",
        "mean, std = evaluate_agent(expert_2mil, env, 10)\n",
        "print(f\"Expert mean return: {mean} +/- {std}\")\n",
        "create_video(vis_env, expert_2mil, \"expert_2mil\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "SvXZhuQQ4QVX"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "# Q4.b Exploring the effect of the effect of the strength of the expert on DAgger\n",
        "We now look at how the strength of the expert affects our imitation learned algorithm.\n",
        "The `expert_1mil` and `expert_2mil` are both policies from the same training run, except\n",
        "the `expert_1mil` was trained for 1 million steps and `expert_2mil` was trained for 2 million steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEJ0FkK04QVX"
      },
      "source": [
        "_Type your answer here, replacing this text._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkodcjz94QVX",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "N_ITERS = 50\n",
        "N_DATA_PER_ITER = 2000\n",
        "N_GRADIENT_STEPS = 2000\n",
        "expert_strength_data = {\n",
        "    \"all_means\": [],\n",
        "    \"all_stds\": [],\n",
        "    \"all_losses\": [],\n",
        "    \"all_xs\": [],\n",
        "}\n",
        "for expert in [expert_1mil, expert_2mil]:\n",
        "    agent = Agent(env.observation_space.shape[0], env.action_space.shape[0])\n",
        "    optimizer = torch.optim.Adam(agent.parameters(), lr=5e-3)\n",
        "    buffer = ReplayBuffer()\n",
        "    seed_data(env, expert, buffer, 2000)\n",
        "\n",
        "    # TODO: Run DAgger for the given expert\n",
        "\n",
        "    xs = np.arange(N_ITERS) + 1\n",
        "    expert_strength_data[\"all_xs\"].append(xs)\n",
        "    expert_strength_data[\"all_means\"].append(means)\n",
        "    expert_strength_data[\"all_stds\"].append(stds)\n",
        "    expert_strength_data[\"all_losses\"].append(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sceVO5ys4QVX",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "plot(\n",
        "    expert_strength_data[\"all_xs\"],\n",
        "    expert_strength_data[\"all_means\"],\n",
        "    expert_strength_data[\"all_stds\"],\n",
        "    expert_strength_data[\"all_losses\"],\n",
        "    [f\"{expert} expert\" for expert in [\"1mil\", \"2mil\"]],\n",
        "    min=-1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "mI67mAiB4QVX"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "## Q4.c Exploring the effect of the number of iterations on DAgger\n",
        "We will now look at how the frequency of the number of DAgger iterations affects the\n",
        "performance. To make it fair, make sure to control for the total amount of data and\n",
        "gradient steps that will be taken by the algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqFTK-q44QVY"
      },
      "source": [
        "_Type your answer here, replacing this text._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEAB79vh4QVY",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "TOTAL_DATA = 100_000\n",
        "TOTAL_GRADIENT_STEPS = 100_000\n",
        "n_iters_data = {\n",
        "    \"all_means\": [],\n",
        "    \"all_stds\": [],\n",
        "    \"all_losses\": [],\n",
        "    \"all_xs\": [],\n",
        "}\n",
        "expert = ExpertAgent(\"./public/a2/experts/network_2mil.pt\")\n",
        "for n_iters in [5, 25, 50, 100, 200]:\n",
        "    agent = Agent(env.observation_space.shape[0], env.action_space.shape[0])\n",
        "    optimizer = torch.optim.Adam(agent.parameters(), lr=5e-3)\n",
        "    buffer = ReplayBuffer()\n",
        "    seed_data(env, expert, buffer, 2000)\n",
        "\n",
        "    # TODO: Run DAgger for n_iters iterations\n",
        "    xs = 100_000 / n_iters * (np.arange(n_iters) + 1)\n",
        "    n_iters_data[\"all_xs\"].append(xs)\n",
        "    n_iters_data[\"all_means\"].append(means)\n",
        "    n_iters_data[\"all_stds\"].append(stds)\n",
        "    n_iters_data[\"all_losses\"].append(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gI9JXtUa4QVY",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "plot(\n",
        "    n_iters_data[\"all_xs\"],\n",
        "    n_iters_data[\"all_means\"],\n",
        "    n_iters_data[\"all_stds\"],\n",
        "    n_iters_data[\"all_losses\"],\n",
        "    [f\"{n_iters} iters\" for n_iters in [5, 25, 50, 100, 200]],\n",
        "    min=-1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "29o3ie2N4QVY"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
